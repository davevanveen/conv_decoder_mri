{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MRI reconstruction from multicoil data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num GPUs 4\n"
     ]
    }
   ],
   "source": [
    "## EDITS: commented out unnecessary imports\n",
    "\n",
    "# from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import sigpy.mri as mr\n",
    "\n",
    "# import sigpy as sp\n",
    "# from os import listdir\n",
    "# from os.path import isfile, join\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from include import *\n",
    "\n",
    "# from PIL import Image\n",
    "# import PIL\n",
    "import h5py\n",
    "from common.evaluate import *\n",
    "from pytorch_msssim import ms_ssim # only to evaluate/compare across convdecoder, dd, dip\n",
    "# import pickle\n",
    "from common.subsample import MaskFunc # only if we need to generate our own mask\n",
    "\n",
    "from DIP_UNET_models.skip import * # only to evaluate/compare across convdecoder, dd, dip\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from include import transforms as transform\n",
    "\n",
    "GPU = True\n",
    "if GPU == True:\n",
    "    torch.backends.cudnn.enabled = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    dtype = torch.cuda.FloatTensor\n",
    "#     os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "    gpu = 0\n",
    "    torch.cuda.set_device(gpu)\n",
    "    print(\"num GPUs\",torch.cuda.device_count())\n",
    "else:\n",
    "    dtype = torch.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "sys.exit() # cell for reference\n",
    "\n",
    "net, ni, slice_ksp_cd = fit_untrained(net, num_channels, mask, \n",
    "                                      in_size, slice_ksp, slice_ksp_torchtensor)\n",
    "# net: convdecoder\n",
    "# num_channels = 160\n",
    "# mask: binary torch tensor of size [1, 1, ~368, 1]\n",
    "# in_size: hard-coded above as [8,4]\n",
    "# slice_ksp: mri measurements in npy as dtype=complex64, e.g. shape (15, 640, 368)\n",
    "# slice_ksp_torchtensor: mri measurements in torch tensor w 2 channels, e.g. shape [15, 640, 368, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_untrained(parnet, num_channels, mask, in_size, slice_ksp, \n",
    "                  slice_ksp_torchtensor):\n",
    "    \n",
    "    # fix the scaling b/w original image and random output image = net(input tensor w values ~U[0,1]) \n",
    "    # note: this can be done using the under-sampled kspace as well, but we do it using the full kspace\n",
    "    scale_out = 1\n",
    "    scaling_factor, ni = get_scale_factor(parnet,\n",
    "                                       num_channels,\n",
    "                                       in_size,\n",
    "                                       slice_ksp,\n",
    "                                       scale_out=scale_out)\n",
    "    # e.g. scaling_factor = 168813\n",
    "    # ni: network input, i.e. tensor w values sampled uniformly on [0,1]\n",
    "    slice_ksp_torchtensor = slice_ksp_torchtensor * scaling_factor\n",
    "    slice_ksp = slice_ksp * scaling_factor\n",
    "    \n",
    "    ### mask the kspace\n",
    "    # note: apply_mask() only genereates new mask if mask arg is blank. else, returns same mask\n",
    "    if slice_ksp_torchtensor.shape[2] != mask.shape[2]: # added to avoid dim error\n",
    "        mask_ = mask[:,:,:slice_ksp_torchtensor.shape[2],:]\n",
    "        masked_kspace, mask = transform.apply_mask(slice_ksp_torchtensor, mask = mask_)\n",
    "    else:    \n",
    "        masked_kspace, mask = transform.apply_mask(slice_ksp_torchtensor, mask = mask)\n",
    "    # convert measurement to torch tensor\n",
    "    unders_measurement = np_to_var(masked_kspace.data.cpu().numpy()).type(dtype)\n",
    "    # do ifft of input measurements\n",
    "    sampled_image2 = transform.ifft2(masked_kspace)\n",
    "    \n",
    "    # unused\n",
    "#     ### ??? compute reconstructed image i.e. ground-truth\n",
    "#     # ksp2measurement(): convert complex npy array into torch tensor w 2 channels\n",
    "#     measurement = ksp2measurement(slice_ksp).type(dtype)\n",
    "#     # lsreconstruction(): take ifft of measurement and return combined magnitude of real/imag parts\n",
    "#     lsimg = lsreconstruction(measurement)\n",
    "    \n",
    "    ### fit the network to the under-sampled measurement\n",
    "    out = []\n",
    "    for img in sampled_image2:\n",
    "        out += [ img[:,:,0].numpy() , img[:,:,1].numpy() ]\n",
    "    lsest = torch.tensor(np.array([out]))\n",
    "    \n",
    "    ssim_list, psnr_list, norm_ratio, mse_wrt_noisy, mse_wrt_truth, net_input, net = fit(net=parnet,\n",
    "                                                                img_noisy_var=unders_measurement,\n",
    "                                                                img_clean_var=Variable(lsest).type(dtype),\n",
    "                                                                num_channels=[num_channels]*(num_layers-1),\n",
    "                                                                net_input = ni,\n",
    "                                                                mask = mask2d,\n",
    "                                                                #lsimg = lsimg,\n",
    "                                                                find_best=True,         \n",
    "                                                                LR=0.008,\n",
    "                                                                num_iter=200, #20000      \n",
    "                                                                scale_out=scale_out # default 1\n",
    "                                                                          )\n",
    "    \n",
    "    return net, net_input, slice_ksp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_consistency(parnet, parni, mask1d, slice_ksp):\n",
    "    ''' actually compute network output \n",
    "        replace the predicted coefficients in k-space by the original coefficient if it has been sampled '''\n",
    "    \n",
    "    # estimate image \\hat{x} = G(\\hat{C})\n",
    "    img = parnet(parni.type(dtype)) # shape: torch.Size([1, 30, 640, 368])\n",
    "    num_slices = int(img.shape[1]/2) # 15*2=30, i.e. real/complex separate\n",
    "    # combine real/complex channels into one complex channel\n",
    "    fimg = Variable(torch.zeros((img.shape[0], num_slices, img.shape[2], img.shape[3], 2))).type(dtype)\n",
    "    for i in range(num_slices):\n",
    "        fimg[0,i,:,:,0] = img[0,2*i,:,:]\n",
    "        fimg[0,i,:,:,1] = img[0,2*i+1,:,:]\n",
    "    \n",
    "    # take fourier transform to convert into k-space; split real/complex parts, transpose dimensions\n",
    "    Fimg = transform.fft2(fimg) # shape: torch.Size([1, 15, 640, 368, 2])\n",
    "    meas = ksp2measurement(slice_ksp) # shape: torch.Size([1, 15, 640, 368, 2]); slice_ksp has shape (15,640,368) complex\n",
    "    \n",
    "    # replace the predicted coeffs in k-space by original coeffs if it has been sampled\n",
    "    mask = torch.from_numpy(np.array(mask1d, dtype=np.uint8)) # shape: torch.Size([368]) w 41 non-zero elements\n",
    "    ksp_dc = Fimg.clone().detach().cpu()\n",
    "    # applying mask, ksp_dc goes from having 7065599 nonzeros to 7065565 nonzeros, i.e. <41 difference\n",
    "    # question: shouldn't this mask zero out entire columns, not just single elements?\n",
    "    ksp_dc[:,:,:,mask==1,:] = meas[:,:,:,mask==1,:] # after data consistency block\n",
    "    \n",
    "    # now we have M*F*G(\\hat{C})\n",
    "\n",
    "    # take ifft of measurements\n",
    "    img_dc = transform.ifft2(ksp_dc)[0]\n",
    "    \n",
    "    # combine 30 channels w real/complex separate to 15 complex imgs\n",
    "    out = []\n",
    "    for img in img_dc.detach().cpu():\n",
    "        out += [ img[:,:,0].numpy() , img[:,:,1].numpy() ]\n",
    "    par_out_chs = np.array(out)\n",
    "    #par_out_chs = parnet( parni.type(dtype),scale_out=scale_out ).data.cpu().numpy()[0]\n",
    "    par_out_imgs = channels2imgs(par_out_chs)\n",
    "\n",
    "    # combine 15 complex imgs via standard rss, then crop center for 320x320 grayscale output\n",
    "    prec = crop_center2(root_sum_of_squares2(par_out_imgs),320,320)\n",
    "    \n",
    "    return prec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_convD = data_consistency(net, ni, mask1d, slice_ksp_cd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading MRI measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kspace shape (number slices, number coils, x, y):  (37, 15, 640, 368)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<KeysViewHDF5 ['ismrmrd_header', 'kspace', 'mask']>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get full stack of slices\n",
    "# note: if from val set, contains kspace meas f['kspace'] and rss recon f['reconstruction_rss']\n",
    "filename = '/bmrNAS/people/dvv/multicoil_test_v2/file1000781_v2.h5'\n",
    "f = h5py.File(filename, 'r') \n",
    "print(\"Kspace shape (number slices, number coils, x, y): \", f['kspace'].shape)\n",
    "\n",
    "# isolate kspace slice\n",
    "slicenu = f[\"kspace\"].shape[0]//2\n",
    "slice_ksp = f['kspace'][slicenu]\n",
    "slice_ksp_torchtensor = transform.to_tensor(slice_ksp)      # Convert from numpy array to pytorch tensor\n",
    "\n",
    "# note - f['reconstruction_rss'] only exists in val set, not test set\n",
    "# kspace dtype is complex64, hence cannot display without conversion\n",
    "\n",
    "# fig = plt.figure(figsize=(6,6))\n",
    "# ax = fig.add_subplot(111)\n",
    "# ax.imshow(f[\"reconstruction_rss\"][slicenu],\"gray\")\n",
    "# ax.set(title=\"ground truth\")\n",
    "# ax.axis(\"off\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load or create mask, M\n",
    "\n",
    "- Load if .h5 file already has one, otherwise create\n",
    "    - Format of loaded mask is 1d binary vector of size ~368\n",
    "- Convert mask to 0's and 1's, zero pad, convert to 2D, create torch transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "under-sampling factor: 8.98\n"
     ]
    }
   ],
   "source": [
    "# try: # if the file already has a mask\n",
    "temp = np.array([1 if e else 0 for e in f[\"mask\"]])\n",
    "temp = temp[np.newaxis].T\n",
    "temp = np.array([[temp]])\n",
    "mask = transform.to_tensor(temp).type(dtype).detach().cpu()\n",
    "# except: # if we need to create a mask\n",
    "#     desired_factor = 4 # desired under-sampling factor\n",
    "#     undersampling_factor = 0\n",
    "#     tolerance = 0.03\n",
    "#     while undersampling_factor < desired_factor - tolerance or undersampling_factor > desired_factor + tolerance:\n",
    "#         mask_func = MaskFunc(center_fractions=[0.07], accelerations=[desired_factor])  # Create the mask function object\n",
    "#         masked_kspace, mask = transform.apply_mask(slice_ksp_torchtensor, mask_func=mask_func)   # Apply the mask to k-space\n",
    "#         mask1d = var_to_np(mask)[0,:,0]\n",
    "#         undersampling_factor = len(mask1d) / sum(mask1d)\n",
    "\n",
    "mask1d = var_to_np(mask)[0,:,0]\n",
    "\n",
    "# The provided mask and data have last dim of 368, but the actual data is smaller.\n",
    "# To prevent the network learning outside the data region, we force the mask to 0 there.\n",
    "mask1d[:mask1d.shape[-1]//2-160] = 0 \n",
    "mask1d[mask1d.shape[-1]//2+160:] =0\n",
    "mask2d = np.repeat(mask1d[None,:], slice_ksp.shape[1], axis=0).astype(int) # Turning 1D Mask into 2D that matches data dimensions\n",
    "mask2d = np.pad(mask2d,((0,),((slice_ksp.shape[-1]-mask2d.shape[-1])//2,)),mode='constant') # Zero padding to make sure dimensions match up\n",
    "mask = transform.to_tensor( np.array( [[mask2d[0][np.newaxis].T]] ) ).type(dtype).detach().cpu()\n",
    "print(\"under-sampling factor:\",round(len(mask1d)/sum(mask1d),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup and fit ConvDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(15, 8), (28, 15), (53, 28), (98, 53), (183, 102), (343, 193), (640, 368)]\n",
      "# parameters of ConvDecoder: 1850560\n"
     ]
    }
   ],
   "source": [
    "arch_name = \"ConvDecoder\"\n",
    "\n",
    "out_depth = slice_ksp.shape[0]*2 # 2*n_c, i.e. 2*15=30 if multi-coil\n",
    "out_size = slice_ksp.shape[1:] # shape of (x,y) image slice, e.g. (640, 368)\n",
    "\n",
    "num_channels = 160 #256\n",
    "num_layers = 8\n",
    "strides = [1]*(num_layers-1)\n",
    "in_size = [8,4]\n",
    "kernel_size = 3\n",
    "\n",
    "net = convdecoder(in_size, out_size, out_depth, num_layers, strides, num_channels, act_fun = nn.ReLU(),\n",
    "                     skips=False, need_sigmoid=False, bias=False, need_last = True,\n",
    "                     kernel_size=kernel_size, upsample_mode=\"nearest\").type(dtype)\n",
    "\n",
    "print(\"# parameters of {}:\".format(arch_name),num_param(net))\n",
    "#print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimize with adam 0.008\n",
      "Iteration 00100    Train loss 0.028007  Actual loss 0.032156 Actual loss orig 0.032156 \r"
     ]
    }
   ],
   "source": [
    "net, ni, slice_ksp_cd = fit_untrained(net, num_channels, mask, in_size, slice_ksp, slice_ksp_torchtensor)\n",
    "# net: convdecoder\n",
    "# num_channels = 160\n",
    "# mask: binary torch tensor of size [1, 1, ~368, 1]\n",
    "# in_size: hard-coded above as [8,4]\n",
    "# slice_ksp: mri measurements in npy as dtype=complex64, e.g. shape (15, 640, 368)\n",
    "# slice_ksp_torchtensor: mri measurements in torch tensor w 2 channels, e.g. shape [15, 640, 368, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 640, 368)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slice_ksp_cd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_convD = data_consistency(net, ni, mask1d, slice_ksp_cd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(320, 320)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec_convD.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup and fit Deep Decoder (DD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### delete cashe\n",
    "del(net,ni)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arch_name = \"DD\"\n",
    "###\n",
    "num_channels = 368\n",
    "num_layers = 10\n",
    "in_size = [16,16]\n",
    "\n",
    "net = skipdecoder(out_size,in_size,output_depth,\n",
    "                   num_layers,num_channels,skips=False,need_last=True,\n",
    "                   need_sigmoid=False,upsample_mode=\"bilinear\").type(dtype)\n",
    "print(\"#prameters of {}:\".format(arch_name),num_param(net))\n",
    "#print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net,ni,slice_ksp_cd = fit_untrained(net, num_channels, mask, in_size, slice_ksp, slice_ksp_torchtensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_DD = data_consistency(net, ni, mask1d, slice_ksp_cd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup and fit Deep Image Prior (DIP) (encoder-decoder style architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### delete cashe\n",
    "del(net,ni)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arch_name = \"DIP\"\n",
    "### \n",
    "in_size = slice_ksp.shape[-2:]\n",
    "pad = \"zero\" #'reflection' # 'zero'\n",
    "num_channels = 256\n",
    "net = skip(in_size,num_channels, output_depth, \n",
    "           num_channels_down = [num_channels] * 8,\n",
    "           num_channels_up =   [num_channels] * 8,\n",
    "           num_channels_skip =    [num_channels*0] * 6 + [4,4],  \n",
    "           filter_size_up = 3, filter_size_down = 5, \n",
    "           upsample_mode='nearest', filter_skip_size=1,\n",
    "           need_sigmoid=False, need_bias=True, pad=pad, act_fun='ReLU').type(dtype)\n",
    "print(\"#prameters of {}:\".format(arch_name),num_param(net))\n",
    "#print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net,ni,slice_ksp_cd = fit_untrained(net, num_channels, mask, in_size, slice_ksp, slice_ksp_torchtensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_DIP = data_consistency(net, ni, mask1d, slice_ksp_cd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scores(im1,im2):\n",
    "    im1 = (im1-im1.mean()) / im1.std()\n",
    "    im1 *= im2.std()\n",
    "    im1 += im2.mean()\n",
    "    \n",
    "    vif_ = vifp_mscale(im1,im2,sigma_nsq=im1.mean())\n",
    "    \n",
    "    ssim_ = ssim(np.array([im1]), np.array([im2]))\n",
    "    psnr_ = psnr(np.array([im1]),np.array([im2]))\n",
    "\n",
    "    dt = torch.FloatTensor\n",
    "    im11 = torch.from_numpy(np.array([[im1]])).type(dt)\n",
    "    im22 = torch.from_numpy(np.array([[im2]])).type(dt)\n",
    "    ms_ssim_ = ms_ssim(im11, im22,data_range=im22.max()).data.cpu().numpy()[np.newaxis][0]\n",
    "    return vif_, ms_ssim_, ssim_, psnr_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = f[\"reconstruction_rss\"][slicenu]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vif_cd, ms_ssim_cd, ssim_cd, psnr_cd  = scores(gt, rec_convD)\n",
    "vif_dd, ms_ssim_dd, ssim_dd, psnr_dd  = scores(gt, rec_DD)\n",
    "vif_dip, ms_ssim_dip, ssim_dip, psnr_dip  = scores(gt, rec_DIP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (16,14)) # create a 5 x 5 figure \n",
    "    \n",
    "ax1 = fig.add_subplot(221)\n",
    "ax1.imshow(gt,cmap='gray')\n",
    "ax1.set_title('Ground Truth')\n",
    "ax1.axis('off')\n",
    "\n",
    "ax2 = fig.add_subplot(222)\n",
    "ax2.imshow(rec_convD,cmap='gray')\n",
    "ax2.set_title( \"ConvDecoder\") \n",
    "ax2.axis('off') \n",
    "\n",
    "ax3 = fig.add_subplot(223)\n",
    "ax3.imshow(rec_DD,cmap='gray')\n",
    "ax3.set_title( \"Deep Decoder\" ) \n",
    "ax3.axis('off')\n",
    "\n",
    "ax4 = fig.add_subplot(224)\n",
    "ax4.imshow(rec_DIP,cmap='gray')\n",
    "ax4.set_title( \"Deep Image Prior\" ) \n",
    "ax4.axis('off')\n",
    "\n",
    "print(\"ConvDecoder       --> VIF: %.2f, MS-SSIM: %.2f, SSIM: %.2f, PSNR: %.2f \" % (vif_cd,ms_ssim_cd,ssim_cd,psnr_cd))\n",
    "print(\"Deep Decoder      --> VIF: %.2f, MS-SSIM: %.2f, SSIM: %.2f, PSNR: %.2f \" % (vif_dd,ms_ssim_dd,ssim_dd,psnr_dd))\n",
    "print(\"Deep Image Prior  --> VIF: %.2f, MS-SSIM: %.2f, SSIM: %.2f, PSNR: %.2f \" % (vif_dip,ms_ssim_dip,ssim_dip,psnr_dip))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
