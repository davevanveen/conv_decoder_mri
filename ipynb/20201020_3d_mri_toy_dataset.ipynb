{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import pydicom\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import sigpy\n",
    "from sigpy.mri.samp import poisson\n",
    "import torch\n",
    "\n",
    "sys.path.append('/home/vanveen/ConvDecoder/')\n",
    "from utils.data_io import load_h5, load_output, save_output, \\\n",
    "                            expmt_already_generated\n",
    "from utils.transform import np_to_tt, split_complex_vals, recon_ksp_to_img\n",
    "from utils.helpers import num_params, get_masks\n",
    "from include.decoder_conv import init_convdecoder\n",
    "from include.mri_helpers import get_scale_factor, get_masked_measurements, \\\n",
    "                                data_consistency\n",
    "from include.fit import fit\n",
    "from utils.evaluate import calc_metrics\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.enabled = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    dtype = torch.cuda.FloatTensor\n",
    "    torch.cuda.set_device(1)\n",
    "    \n",
    "from utils.transform import np_to_tt, np_to_var, apply_mask, ifft_2d, fft_2d, \\\n",
    "                        reshape_complex_channels_to_sep_dimn, \\\n",
    "                        reshape_complex_channels_to_be_adj, \\\n",
    "                        split_complex_vals, recon_ksp_to_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "164\n"
     ]
    }
   ],
   "source": [
    "path = '/bmrNAS/people/arjun/data/qdess_knee_2020/files_recon_calib-16/'\n",
    "files = [f for f in listdir(path) if isfile(join(path, f))]\n",
    "files.sort()\n",
    "\n",
    "print(len(files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### data format\n",
    "- 'kspace': Nx x Ny x Nz x # echos x # coils\n",
    "- 'maps': Nx x Ny x Nz x # coils x # maps\n",
    "- 'target': Nx x Ny x Nz x # echos x # maps\n",
    "\n",
    "take kspace, run on one echo. what to do w num coils? recon all, then rss at end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load data, make mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.cuda.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fn = files[0]\n",
    "# f = h5py.File(path + fn, 'r')\n",
    "\n",
    "# ksp = f['kspace']\n",
    "# targ = f['target']\n",
    "\n",
    "# slice_ksp_file = ksp.value\n",
    "# np.save('slice_ksp_file.npy', slice_ksp_file)\n",
    "slice_ksp_file = np.load('slice_ksp_file.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512, 160)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: pre-load this since it takes a long time to generate\n",
    "# mask = poisson(img_shape=(512, 160), accel=4)\n",
    "# mask = abs(mask)\n",
    "# np.save('mask_3d.npy', mask)\n",
    "mask = np.load('mask_3d.npy').astype('float32')\n",
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 20)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for faster processing, downsample slice_ksp and mask\n",
    "\n",
    "slice_ksp_ = slice_ksp_file\n",
    "\n",
    "C = 4 # downsample by a factor of four\n",
    "i1, i2, i3 = slice_ksp_.shape[0]//2, slice_ksp_.shape[1]//2, slice_ksp_.shape[2]//2\n",
    "slice_ksp_ = slice_ksp_[i1-i1//C:i1+i1//C, i2-i2//C:i2+i2//C, i3-i3//8:i3+i3//8, :, 6:10]\n",
    "\n",
    "mask = mask[i2-i2//C:i2+i2//C, i3-i3//8:i3+i3//8]\n",
    "mask.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### for now: (1) collapse kz (2) choose one of two echos\n",
    "\n",
    "need another version of slice_ksp, `vol_ksp`, for when we actually apply the mask. then afterward, collapse kz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 128, 20, 2, 4)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slice_ksp_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 128, 128, 20)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vol_ksp = slice_ksp_[:,:,:,0,:]\n",
    "vol_ksp = np.moveaxis(vol_ksp, -1, 0)\n",
    "vol_ksp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 128, 128)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slice_ksp = slice_ksp_[:,:,slice_ksp_.shape[2]//2,0,:]\n",
    "slice_ksp = np.moveaxis(slice_ksp, -1, 0)\n",
    "slice_ksp.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### follow same process as 2d case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-156986220-31252188j) (154675800-93950376j) (155094.81-567062.1j)\n",
      "(-5.966051-1.1876975j) (5.878247-3.570458j) (0.0058941706-0.021550436j)\n"
     ]
    }
   ],
   "source": [
    "print(slice_ksp.min(), slice_ksp.max(), slice_ksp.mean())\n",
    "net, net_input, slice_ksp = init_convdecoder(slice_ksp, mask)\n",
    "print(slice_ksp.min(), slice_ksp.max(), slice_ksp.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: network has same num_params as original network w lone difference of\n",
    "# 32 = 2*n_c output channels instead of 30\n",
    "# hence as written now, network is agnostic to number of pixels in a slice, e.g. 512x512\n",
    "\n",
    "# from utils.helpers import num_params\n",
    "# params = [p.shape for p in net.parameters()]\n",
    "# # params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 128, 128, 2])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_gt = recon_ksp_to_img(slice_ksp, dim=128)\n",
    "\n",
    "ksp_orig = np_to_tt(split_complex_vals(slice_ksp))[None, :].type(dtype)\n",
    "ksp_orig.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4, 128, 128, 20), (128, 20))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vol_ksp.shape, mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 1, 128, 20, 1), dtype('float32'))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_ = mask[np.newaxis, np.newaxis, :, :, np.newaxis]\n",
    "# mask_ = mask_.astype(dtype)\n",
    "mask_.shape, mask_.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 128, 128, 20, 2])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# need to use 3d (non-collapsed data) for this step then collapse it back down\n",
    "# currently assumes vol_ksp is [16, 512, 512, 160]\n",
    "\n",
    "# currently doing a modified version of the below function\n",
    "# ksp_masked, img_masked = get_masked_measurements(vol_ksp, mask_)\n",
    "\n",
    "ksp_masked = apply_mask(np_to_tt(vol_ksp), mask=mask_).type(torch.float32)\n",
    "ksp_masked = np_to_var(ksp_masked.data.cpu().numpy()).type(dtype)\n",
    "ksp_masked = ksp_masked[:, :, :, :, :, :]\n",
    "ksp_masked.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 128, 128])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# currently works only if ksp_masked is downsampled to one slice in z\n",
    "img_masked = ifft_2d(ksp_masked[0]).cpu().numpy()\n",
    "img_masked = reshape_complex_channels_to_be_adj(img_masked)\n",
    "img_masked = np_to_var(img_masked).type(dtype)\n",
    "img_masked.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 128, 128, 2]) torch.Size([1, 8, 128, 128]) (128, 20)\n",
      "torch.Size([1, 8, 128, 128]) (128, 20)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (128) must match the size of tensor b (20) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-73e15b42e7d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mksp_masked\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_masked\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m net, mse_wrt_ksp, mse_wrt_img = fit(\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mksp_masked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mksp_masked\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_masked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimg_masked\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         net=net, net_input=net_input, mask2d=mask, num_iter=10)\n",
      "\u001b[0;32m~/ConvDecoder/include/fit.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(ksp_masked, img_masked, net, net_input, mask2d, mask1d, ksp_orig, DC_STEP, alpha, num_iter, lr, img_ls, dtype, c_wmse)\u001b[0m\n\u001b[1;32m    132\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mloss_ksp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;31m# at each iteration, check if loss improves by 1%. if so, a new best net\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/heck/lib/python3.8/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/heck/lib/python3.8/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mclosure\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mgroup\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ConvDecoder/include/fit.py\u001b[0m in \u001b[0;36mclosure\u001b[0;34m()\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask2d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m             \u001b[0mout_ksp_masked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforwardm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask2d\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mDC_STEP\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ConvDecoder/include/mri_helpers.py\u001b[0m in \u001b[0;36mforwardm\u001b[0;34m(img, mask)\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0mFimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfft2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfimg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# dim: (1,num_slices,x,y,2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0mFimg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m         \u001b[0mFimg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mFimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (128) must match the size of tensor b (20) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "# mask is originally 2d - use this version\n",
    "\n",
    "# original 2d code works with the following shape outputs\n",
    "# torch.Size([1, 15, 640, 372, 2]) torch.Size([1, 30, 640, 372]) (640, 372)\n",
    "# with out = net(net_input) of size [1, 30, 640, 372]\n",
    "# because mask is in the (x,y) plane. here mask is in the (y,z) plane\n",
    "# need to sort out dimensions upstream in order to debug this\n",
    "\n",
    "print(ksp_masked.shape, img_masked.shape, mask.shape)\n",
    "net, mse_wrt_ksp, mse_wrt_img = fit(\n",
    "        ksp_masked=ksp_masked, img_masked=img_masked,\n",
    "        net=net, net_input=net_input, mask2d=mask, num_iter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### how to get sampled k-space from 3d --> 2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ifft2(data):\n",
    "    \"\"\"\n",
    "    Apply centered 2-dimensional Inverse Fast Fourier Transform.\n",
    "    Args:\n",
    "        data (torch.Tensor): Complex valued input data with the last dimension containing\n",
    "            real and imaginary components.\n",
    "        dims (2-tuple): Containing spatial dimension indices.\n",
    "    Returns:\n",
    "        torch.Tensor: The IFFT of the input.\n",
    "    \"\"\"\n",
    "    assert data.size(-1) == 2\n",
    "    ndims = len(list(data.size()))\n",
    "\n",
    "    if ndims == 5:\n",
    "        data = data.permute(0,3,1,2,4)\n",
    "    elif ndims == 6:\n",
    "        data = data.permute(0,3,4,1,2,5)\n",
    "    else:\n",
    "        raise ValueError('ifft2: ndims > 6 not supported!')\n",
    "\n",
    "    data = ifftshift(data, dim=(-3, -2))\n",
    "    data = torch.ifft(data, 2, normalized=True)\n",
    "    data = fftshift(data, dim=(-3, -2))\n",
    "\n",
    "    if ndims == 5:\n",
    "        data = data.permute(0,2,3,1,4)\n",
    "    elif ndims == 6:\n",
    "        data = data.permute(0,3,4,1,2,5)\n",
    "    else:\n",
    "        raise ValueError('ifft2: ndims > 6 not supported!')\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 128, 128, 20, 2])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.transform import fftshift, ifftshift\n",
    "\n",
    "arr = ksp_masked[0]\n",
    "\n",
    "arr = ifftshift(arr, dim=(1,2))\n",
    "arr = torch.ifft(arr, 3)\n",
    "arr = fftshift(arr, dim=(1,2))\n",
    "\n",
    "arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 128, 128, 20, 2])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 128, 128, 20, 2])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr.permute(0,3,1,2,4).permute(0,2,3,1,4).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 128)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr.shape[-4], arr.shape[-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### toy example of arr * mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m4 = abs(poisson(img_shape=(5,4), accel=2))\n",
    "m4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ksp_t = np.ones((5,5,4,2,6), dtype=np.complex64)\n",
    "ksp_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m4_ = m4[np.newaxis, :, :, np.newaxis, np.newaxis]\n",
    "m4_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = ksp_t * m4_\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_cols = 4\n",
    "num_rows = len(tt) // num_cols\n",
    "fig = plt.figure(figsize=(10*num_cols,10*num_rows))\n",
    "\n",
    "for idx, img in enumerate(tt):#num_rows*num_cols]):\n",
    "#     print(img.min(), img.max())\n",
    "    ax = fig.add_subplot(num_rows, num_cols, idx+1)\n",
    "    ax.imshow(img, cmap='gray')\n",
    "    ax.axis('off')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(tt, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### next: figure out how to do sampling e.g. poisson disc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/bmrNAS/people/akshay/dicoms/philips_ge/ge_042719/5889_04272019/001/'\n",
    "files = [f for f in listdir(path) if isfile(join(path, f))]\n",
    "files.sort()\n",
    "\n",
    "print(len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vol_img = []\n",
    "\n",
    "for f in files:\n",
    "\n",
    "    dcm = pydicom.dcmread(path + f)\n",
    "    vol_img.append(dcm.pixel_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = 4\n",
    "num_rows = len(files) // num_cols\n",
    "fig = plt.figure(figsize=(10*num_cols,10*num_rows))\n",
    "\n",
    "for idx, img in enumerate(vol_img[0:num_rows*num_cols]):\n",
    "#     print(img.min(), img.max())\n",
    "    ax = fig.add_subplot(num_rows, num_cols, idx+1)\n",
    "    ax.imshow(img, cmap='gray')\n",
    "    ax.axis('off')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_cols = 4\n",
    "num_rows = len(files) // num_cols\n",
    "fig = plt.figure(figsize=(10*num_cols,10*num_rows))\n",
    "\n",
    "for idx, img in enumerate(vol_img[0:num_rows*num_cols]):\n",
    "#     print(img.min(), img.max())\n",
    "    ax = fig.add_subplot(num_rows, num_cols, idx+1)\n",
    "    ax.imshow(img, cmap='gray')\n",
    "    ax.axis('off')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
