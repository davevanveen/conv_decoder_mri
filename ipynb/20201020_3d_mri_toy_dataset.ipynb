{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import pydicom\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import sigpy\n",
    "from sigpy.mri.samp import poisson\n",
    "import torch\n",
    "from torch.fft import ifftn\n",
    "\n",
    "sys.path.append('/home/vanveen/ConvDecoder/')\n",
    "from utils.data_io import load_h5, load_output, save_output, \\\n",
    "                            expmt_already_generated\n",
    "from utils.transform import np_to_tt, split_complex_vals, recon_ksp_to_img, ifft_2d\n",
    "from utils.helpers import num_params, get_masks\n",
    "from include.decoder_conv import init_convdecoder\n",
    "from include.mri_helpers import get_scale_factor, get_masked_measurements, \\\n",
    "                                data_consistency\n",
    "from include.fit import fit\n",
    "from utils.evaluate import calc_metrics\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.enabled = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    dtype = torch.cuda.FloatTensor\n",
    "    torch.cuda.set_device(1)\n",
    "    \n",
    "from utils.transform import np_to_tt, np_to_var, apply_mask, ifft_2d, fft_2d, \\\n",
    "                        reshape_complex_channels_to_sep_dimn, \\\n",
    "                        reshape_complex_channels_to_be_adj, \\\n",
    "                        split_complex_vals, recon_ksp_to_img, \\\n",
    "                        fftshift, ifftshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "164\n"
     ]
    }
   ],
   "source": [
    "path = '/bmrNAS/people/arjun/data/qdess_knee_2020/files_recon_calib-16/'\n",
    "files = [f for f in listdir(path) if isfile(join(path, f))]\n",
    "files.sort()\n",
    "\n",
    "print(len(files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### data format\n",
    "- 'kspace': Nx x Ny x Nz x # echos x # coils\n",
    "- 'maps': Nx x Ny x Nz x # coils x # maps\n",
    "- 'target': Nx x Ny x Nz x # echos x # maps\n",
    "\n",
    "take kspace, run on one echo. what to do w num coils? recon all, then rss at end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load data, make mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.cuda.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 512, 512, 160])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fn = files[0]\n",
    "f = h5py.File(path + fn, 'r')\n",
    "\n",
    "# ksp = torch.from_numpy(f['kspace'][()])\n",
    "# targ = torch.from_numpy(f['target'][()])\n",
    "ksp = torch.from_numpy(np.load('ksp_3d_samp.npy'))\n",
    "\n",
    "# get echo1, reshape to be (nc, kx, ky, kz)\n",
    "ksp_vol = ksp[:,:,:,0,:].permute(3,0,1,2)\n",
    "ksp_vol.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 160]) torch.Size([1, 512, 160])\n"
     ]
    }
   ],
   "source": [
    "# mask = poisson(img_shape=(512, 160), accel=4)\n",
    "# mask = abs(mask)\n",
    "# np.save('mask_3d.npy', mask)\n",
    "mask = torch.from_numpy(np.load('mask_3d.npy').astype('float32'))\n",
    "mask.shape\n",
    "\n",
    "# change dimensions of mask to multiply with volume\n",
    "mask_ = mask[np.newaxis, :, :]\n",
    "print(mask.shape, mask_.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get central slice in kx of volumes, apply mask\n",
    "because dd+ requires a 2d recon, and we're undersampling in ky, kz\n",
    "\n",
    "currently a modified version of the function call `ksp_masked, img_masked = get_masked_measurements(vol_ksp, mask_)` which has a bunch of data shape conversion nonsense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_kx = ksp_vol.shape[1] // 2\n",
    "ksp_slice = ksp_vol[:, idx_kx, :, :]\n",
    "\n",
    "ksp_masked = ksp_slice * mask_\n",
    "img_masked = ifft_2d(ksp_masked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: \n",
    "- reshape the ksp_masked, img_masked variables according to what fit() requires\n",
    "- first re-write fit() so data types/shape makes sense, are tensors\n",
    "    - do this step-by-step w fastmri dataset since that already works? or get it working w qdess, then merge into fastmri processing?\n",
    "    \n",
    "### do all array processing in torch!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### quick conversion to run fit() w original data format [delete or re-format later]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 512, 160, 2]) torch.Size([1, 32, 512, 160])\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "# original code [1, 15, 640, 372, 2], [1, 30, 640, 372], mask=(640, 372)\n",
    "# ksp_masked want (1, 16, 512, 160, 2), img_masked want (1, 32, 512, 160)\n",
    "\n",
    "ksp_masked_ = np_to_var(ksp_masked).type(dtype)\n",
    "\n",
    "img_masked_ = torch.cat([torch.real(img_masked), torch.imag(img_masked)])\n",
    "img_masked_ = Variable(img_masked_[None, :]).type(dtype)\n",
    "\n",
    "print(ksp_masked_.shape, img_masked_.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### initialize network\n",
    "\n",
    "network has same num_params as original network w lone difference of 32 = 2 * n_c output channels instead of 30. hence as written now, network is agnostic to number of pixels in a slice, e.g. 512x512 would have same num_params as 512x160 -- is this right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slice_ksp (nc, x, y) in original. now slice_ksp (nc, y, z)\n",
    "# mask is mask2d is (x,y) in original. now (y,z)\n",
    "net, net_input, slice_ksp = init_convdecoder(ksp_slice, mask)\n",
    "\n",
    "# from utils.helpers import num_params\n",
    "# params = [p.shape for p in net.parameters()]\n",
    "# params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### later todo's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_gt = recon_ksp_to_img(slice_ksp, dim=???)\n",
    "\n",
    "# only need this if doing dc step\n",
    "# ksp_orig = np_to_tt(split_complex_vals(slice_ksp))[None, :].type(dtype)\n",
    "# ksp_orig.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO:\n",
    "\n",
    "follow the trace below. get pytorch==1.7 version fft/ifft to work given exact same shaped inputs. then remove unnecessary data formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 512, 160, 2]) torch.Size([1, 32, 512, 160]) torch.Size([512, 160])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vanveen/ConvDecoder/utils/transform.py:67: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at  /pytorch/aten/src/ATen/native/Copy.cpp:162.)\n",
      "  arr_out[i,:,:,0] = arr[2*i,:,:]\n"
     ]
    }
   ],
   "source": [
    "# with out = net(net_input) of size [1, 30, 640, 372]\n",
    "# because mask is in the (x,y) plane. here mask is in the (y,z) plane\n",
    "\n",
    "print(ksp_masked_.shape, img_masked_.shape, mask.shape)\n",
    "\n",
    "net, mse_wrt_ksp, mse_wrt_img = fit(\n",
    "        ksp_masked=ksp_masked_, img_masked=img_masked_,\n",
    "        net=net, net_input=net_input, mask2d=np.array(mask), num_iter=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get output, apply data consistency\n",
    "\n",
    "follow along w `data_consistency()` in include.mrihelpers\n",
    "\n",
    "# ISSUE\n",
    "- img_out is 32-length real-valued, but fft_2d(img_out) is 32-length complex-valued \n",
    "- perhaps i can only do fft() on complex-valued signal? or can i also do fft() on real-valued signal?\n",
    "- questions to answer\n",
    "    - how did i do it w fastmri? input and output shape to fft/ifft was [x,y,2], i.e. old torch version did this separately. how does new torch version fft do it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_complex(arr):\n",
    "    dt = arr.dtype\n",
    "    return dt==torch.complex64 or dt==torch.complex128 or dt==torch.complex32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 512, 160]) False\n"
     ]
    }
   ],
   "source": [
    "img_out = net(net_input.type(dtype))[0]\n",
    "print(img_out.shape, is_complex(img_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 512, 160]) True\n"
     ]
    }
   ],
   "source": [
    "ksp_est = fft_2d(img_out)\n",
    "print(ksp_est.shape, is_complex(ksp_est))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 512, 160]), True)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slice_ksp.shape, is_complex(slice_ksp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
