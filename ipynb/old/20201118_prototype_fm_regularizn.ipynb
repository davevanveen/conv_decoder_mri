{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook Purpose\n",
    "\n",
    "prototype dc regularization of intermediate layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import h5py\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import PIL\n",
    "\n",
    "sys.path.append('/home/vanveen/ConvDecoder/')\n",
    "from utils.data_io import load_h5, get_mask, num_params\n",
    "from include.decoder_conv import init_convdecoder\n",
    "from include.decoder_conv import get_scale_factor, get_net_input, get_hidden_size\n",
    "from include.fit import fit\n",
    "from include.subsample import MaskFunc\n",
    "from utils.evaluate import calc_metrics\n",
    "from utils.transform import fft_2d, ifft_2d, root_sum_squares, \\\n",
    "                            reshape_complex_vals_to_adj_channels, \\\n",
    "                            reshape_adj_channels_to_complex_vals, \\\n",
    "                            crop_center\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.enabled = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    dtype = torch.cuda.FloatTensor\n",
    "    torch.cuda.set_device(0)\n",
    "else:\n",
    "    dtype = torch.FloatTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STATUS\n",
    "\n",
    "### done:\n",
    "- done: extracted feature maps\n",
    "    - actually getting collapsed channels from original feature maps, i.e. 160 --> 30 via 1x1 conv\n",
    "    - we'll be computing mse w dimn [2*nc, x_, y_]\n",
    "- applied a mask to the feat_map\n",
    "    - ksp_m_down, shape [x_,y_], has many zero values. feat_map doesn't have zero values\n",
    "    - we compute mse(feat_map, ksp_m_down) for each layer\n",
    "    - do we want to penalize zero values in ksp_m_down? i don't think so\n",
    "        - we just don't have a prior at those indices. the true value is surely not zero\n",
    "        - if we did penalize perfectly, we'd recreate ksp_masked. want to allow the network expressive freedom to re-create ksp_orig\n",
    "- normalized ksp_m_down according to feat_map so we maintain distribution of feat_map\n",
    "- performed weighted mse so we apply the same weight to each pixel\n",
    "    - last layer has 2000x as many pixels as first layer\n",
    "\n",
    "### possible implementation todo's\n",
    "- try only applying feat_map_loss in last x iterations\n",
    "- try weighting earlier/later layers more heavily\n",
    "- currently we downsample ksp_masked according to the size of each hidden layer's feat_map\n",
    "    - instead, we could upsample each feat_map and compute its loss with the original-size ksp_masked\n",
    "        - this would encourage network to learn upsampling according to upsample_mode='nearest' \n",
    "- could use different downsampling methods to create ksp_m_down\n",
    "    - currently using most expressive method, i.e. bicubic\n",
    "    - other options\n",
    "        - nearest, i.e. inverse method used by dd+ for upsampling\n",
    "        - bilinear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scaling this up over a bunch of settings\n",
    "\n",
    "- make a dataframe where indiv row is one config setting, indiv column is one sample\n",
    "- rows, i.e. config attributes, according to one run_id\n",
    "    - num_iter\n",
    "        - total\n",
    "        - at which we turn on fm_loss, i.e. 0.5 * total and 0.8 * total\n",
    "    - alpha_fm = 10 ** exp\n",
    "    - weighting: early, late, or all\n",
    "    - downsampling method: nearest, bilinear, bicubic\n",
    "- each entry will be one sample's scores as a tuple w (ssim, psnr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TIME CALCS - number of hours\n",
    "\n",
    "[3 file_ids] * [3 weighting_options] * [3 downsamp_options] * [3 iter_start_fm_loss] * [6 alpha_fm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24.3"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = 81*6 # total runs\n",
    "x = x / 2 # total hours if 10000 iter per run\n",
    "x = x / 2 # split up on two gpu's\n",
    "x = x * (2000 / 10000) # if i do 2000 iter\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "DIM = 320\n",
    "SCALE_FAC = 0.1\n",
    "NUM_ITER = 1\n",
    "\n",
    "file_id_list = ['1000273']#, '1000325', '1000464', '1000007', '1000537', '1000818', \\\n",
    "#                  '1001140', '1001219', '1001338', '1001598', '1001533', '1001798']\n",
    "file_id_list.sort()\n",
    "\n",
    "exp_list = [-1]#-2, -3, -4, -5, -10] # i.e. loss = loss_ksp + 10**exp * loss_feat_map\n",
    "\n",
    "imgs_run_list = []\n",
    "\n",
    "path_out = '/bmrNAS/people/dvv/out_fastmri/expmt_fm_loss/'\n",
    "\n",
    "for file_id in file_id_list:\n",
    "\n",
    "    f, ksp_orig = load_h5(file_id)\n",
    "    ksp_orig = torch.from_numpy(ksp_orig)\n",
    "\n",
    "    mask = get_mask(ksp_orig)\n",
    "    \n",
    "    for exp in exp_list:\n",
    "\n",
    "        net, net_input, ksp_orig_, hidden_size = init_convdecoder(ksp_orig, mask)\n",
    "\n",
    "        ksp_masked = SCALE_FAC * ksp_orig_ * mask\n",
    "        img_masked = ifft_2d(ksp_masked)\n",
    "\n",
    "        net, mse_wrt_ksp, mse_wrt_img = fit(\n",
    "            ksp_masked=ksp_masked, img_masked=img_masked,\n",
    "            net=net, net_input=net_input, mask2d=mask, num_iter=NUM_ITER,\\\n",
    "            alpha_fm=10**exp)\n",
    "\n",
    "\n",
    "        # use above two lines when we don't want to access feature maps\n",
    "        img_out = net(net_input.type(dtype))\n",
    "        img_out = img_out[0] if type(img_out) is tuple else img_out\n",
    "    #     img_out, feat_maps = net(net_input.type(dtype))\n",
    "\n",
    "        img_out = reshape_adj_channels_to_complex_vals(img_out[0])\n",
    "        ksp_est = fft_2d(img_out)\n",
    "        ksp_dc = torch.where(mask, ksp_masked, ksp_est)\n",
    "\n",
    "        img_masked = crop_center(root_sum_squares(ifft_2d(ksp_masked)).detach(), DIM, DIM)\n",
    "        img_est = crop_center(root_sum_squares(ifft_2d(ksp_est)).detach(), DIM, DIM)\n",
    "        img_dc = crop_center(root_sum_squares(ifft_2d(ksp_dc)).detach(), DIM, DIM)\n",
    "        img_gt = crop_center(root_sum_squares(ifft_2d(ksp_orig)), DIM, DIM)\n",
    "        imgs_run_list.append([img_masked, img_est, img_dc, img_gt])\n",
    "        \n",
    "        \n",
    "        _, _, ssim_est, psnr_est = calc_metrics(np.array(img_est), np.array(img_gt))\n",
    "        _, _, ssim_dc, psnr_dc = calc_metrics(np.array(img_dc), np.array(img_gt))\n",
    "\n",
    "    #     np.save('{}{}_est.npy'.format(path_out, file_id), img_est)\n",
    "    #     np.save('{}{}_dc.npy'.format(path_out, file_id), img_dc)\n",
    "    #     np.save('{}{}_gt.npy'.format(path_out, file_id), img_gt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
