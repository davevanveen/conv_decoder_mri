{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Purpose\n",
    "\n",
    "Recreate deep decoder experiments run in `ConvDecoder_vs_DIP_vs_DD_multicoil.ipynb`, hereon referred to as the original notebook, which was extremely messy and unnecessarily complicated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import h5py\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "from utils.transform import to_tensor, to_np, np_to_var, apply_mask, ifft_2d\n",
    "from utils.helpers import num_params\n",
    "from include.decoder_conv import convdecoder\n",
    "from include.mri_helpers import get_scale_factor\n",
    "from include.helpers import np_to_var\n",
    "\n",
    "# TODO: fix these imports\n",
    "# from include import * \n",
    "# from include import transforms as transform\n",
    "# from common.evaluate import * \n",
    "# from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.enabled = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    dtype = torch.cuda.FloatTensor\n",
    "    torch.cuda.set_device(0)\n",
    "#     print(\"num GPUs\",torch.cuda.device_count())\n",
    "else:\n",
    "    dtype = torch.FloatTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load MRI measurements, y\n",
    "\n",
    "Isolate individual 2D slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k-space shape (num_slices, num_coils, x, y):  (37, 15, 640, 368)\n"
     ]
    }
   ],
   "source": [
    "filename = '/bmrNAS/people/dvv/multicoil_test_v2/file1000781_v2.h5'\n",
    "f = h5py.File(filename, 'r') \n",
    "# print('h5 file keys: ', f.keys())\n",
    "print('k-space shape (num_slices, num_coils, x, y): ', f['kspace'].shape)\n",
    "\n",
    "# isolate central k-space slice\n",
    "slice_idx = f['kspace'].shape[0] // 2\n",
    "ksp_slice = f['kspace'][slice_idx]\n",
    "# note: didn't add tensor version e.g. slice_ksp_torchtensor in original"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load mask, M\n",
    "\n",
    "- Format of loaded mask is 1d binary vector of size ~368, i.e. sampling of vertical lines in image\n",
    "- Convert mask to 0's and 1's, zero pad, convert to 2D, create torch transform\n",
    "\n",
    "Note: See original notebook for generating a new mask, e.g. if .h5 doesn't have a mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "under-sampling factor: 8.98\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    mask1d = np.array([1 if e else 0 for e in f[\"mask\"]]) # load 1D binary mask\n",
    "except:\n",
    "    print('Implement method for generating a mask')\n",
    "    sys.exit()\n",
    "    \n",
    "# zero out mask in outer regions e.g. mask and data have last dimn 368, but actual data is size 320\n",
    "# TODO: if actual data is size 320, then why do we have dimn 368?\n",
    "idxs_zero = (mask1d.shape[-1] - 320) // 2 # e.g. zero first/last (368-320)/2=24 indices\n",
    "mask1d[:idxs_zero], mask1d[-idxs_zero:] = 0, 0\n",
    "\n",
    "# create 2d mask. zero pad if dimensions don't line up - is this necessary?\n",
    "mask2d = np.repeat(mask1d[None,:], ksp_slice.shape[1], axis=0)#.astype(int)\n",
    "mask2d = np.pad(mask2d, ((0,),((ksp_slice.shape[-1]-mask2d.shape[-1])//2,)), mode='constant')\n",
    "\n",
    "# convert shape e.g. (368,) --> (1, 1, 368, 1)\n",
    "mask = to_tensor(np.array([[mask2d[0][np.newaxis].T]])).type(torch.FloatTensor)\n",
    "print('under-sampling factor:', round(len(mask1d) / sum(mask1d), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up ConvDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(15, 8), (28, 15), (53, 28), (98, 53), (183, 102), (343, 193), (640, 368)]\n",
      "# parameters of ConvDecoder: 1850560\n"
     ]
    }
   ],
   "source": [
    "arch_name = 'ConvDecoder'\n",
    "\n",
    "in_size = [8,4]\n",
    "out_size = ksp_slice.shape[1:] # shape of (x,y) image slice, e.g. (640, 368)\n",
    "out_depth = ksp_slice.shape[0]*2 # 2*n_c, i.e. 2*15=30 if multi-coil\n",
    "num_layers = 8\n",
    "strides = [1]*(num_layers-1)\n",
    "num_channels = 160\n",
    "kernel_size = 3\n",
    "\n",
    "net = convdecoder(in_size, out_size, out_depth, num_layers, strides, num_channels).type(dtype)\n",
    "\n",
    "print('# parameters of {}:'.format(arch_name),num_params(net))\n",
    "#print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit ConvDecoder\n",
    "\n",
    "# TODO: write this sequentially, e.g. outside of function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([15, 640, 368, 2])\n"
     ]
    }
   ],
   "source": [
    "# fix the scaling b/w original image and random output image = net(input tensor w values ~U[0,1]) \n",
    "# e.g. scaling_factor = 168813\n",
    "# note: can be done using the under-sampled kspace, but we use the full kspace\n",
    "scaling_factor, net_input = get_scale_factor(net,\n",
    "                                   num_channels,\n",
    "                                   in_size,\n",
    "                                   ksp_slice)\n",
    "ksp_slice = ksp_slice * scaling_factor # original fit_untrained() f'n returns this\n",
    "ksp_slice_tt = to_tensor(ksp_slice)\n",
    "    \n",
    "# mask the kspace\n",
    "ksp_masked_tt = apply_mask(ksp_slice_tt, mask=mask)\n",
    "    \n",
    "# convert to torch variable [C, W, H] --> [1, C, W, H]\n",
    "ksp_masked_tt = np_to_var(ksp_masked_tt.data.cpu().numpy()).type(dtype)\n",
    "\n",
    "# perform ifft of masked kspace\n",
    "img_masked_tt = ifft_2d(ksp_masked_tt)\n",
    "\n",
    "### TODO: start here\n",
    "out = []\n",
    "for img in sampled_image2:\n",
    "    out += [ img[:,:,0].numpy() , img[:,:,1].numpy() ]\n",
    "lsest = torch.tensor(np.array([out]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform data consistency step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
