{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Purpose\n",
    "\n",
    "Recreate deep decoder experiments run in `ConvDecoder_vs_DIP_vs_DD_multicoil.ipynb`, hereon referred to as the original notebook, which was extremely messy and unnecessarily complicated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import h5py\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "from utils.transform import np_to_tt, np_to_var, apply_mask, ifft_2d, fft_2d, \\\n",
    "                            reshape_complex_channels_to_sep_dimn, \\\n",
    "                            reshape_complex_channels_to_be_adj, \\\n",
    "                            split_complex_vals, combine_complex_channels, \\\n",
    "                            crop_center, root_sum_of_squares, recon_ksp_to_img\n",
    "from utils.helpers import num_params, load_h5, get_masks\n",
    "from include.decoder_conv import convdecoder\n",
    "from include.mri_helpers import get_scale_factor\n",
    "from include.fit import fit\n",
    "from utils.evaluate import calc_metrics\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.enabled = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    dtype = torch.cuda.FloatTensor\n",
    "    torch.cuda.set_device(1)\n",
    "#     print(\"num GPUs\",torch.cuda.device_count())\n",
    "else:\n",
    "    dtype = torch.FloatTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up ConvDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_convdecoder(slice_ksp, mask):\n",
    "    ''' parameters: \n",
    "                slice_ksp: original, unmasked k-space measurements\n",
    "                mask: mask used to downsample original k-space\n",
    "        return:\n",
    "                net: initialized convdecoder\n",
    "                net_input: random, scaled input seed \n",
    "                ksp_masked: masked measurements to fit\n",
    "                img_masked: masked image, i.e. ifft(ksp_masked) '''\n",
    "\n",
    "    in_size = [8,4]\n",
    "    out_size = slice_ksp.shape[1:] # shape of (x,y) image slice, e.g. (640, 368)\n",
    "    out_depth = slice_ksp.shape[0]*2 # 2*n_c, i.e. 2*15=30 if multi-coil\n",
    "    num_layers = 8\n",
    "    strides = [1]*(num_layers-1)\n",
    "    num_channels = 160\n",
    "    kernel_size = 3\n",
    "\n",
    "    net = convdecoder(in_size, out_size, out_depth, num_layers, \\\n",
    "                      strides, num_channels).type(dtype)\n",
    "    print('# parameters of ConvDecoder:',num_params(net))\n",
    "    \n",
    "    # generate network input\n",
    "    # fix scaling b/w original image and network output\n",
    "    scale_factor, net_input = get_scale_factor(net,\n",
    "                                       num_channels,\n",
    "                                       in_size,\n",
    "                                       slice_ksp)\n",
    "    slice_ksp = slice_ksp * scale_factor # original fit_untrained() f'n returns this\n",
    "\n",
    "    # mask the kspace\n",
    "    ksp_masked = apply_mask(np_to_tt(slice_ksp), mask=mask)\n",
    "    ksp_masked = np_to_var(ksp_masked.data.cpu().numpy()).type(dtype)\n",
    "\n",
    "    # perform ifft of masked kspace\n",
    "    img_masked = ifft_2d(ksp_masked[0]).cpu().numpy()\n",
    "    img_masked = reshape_complex_channels_to_be_adj(img_masked)\n",
    "    img_masked = np_to_var(img_masked).type(dtype)\n",
    "        \n",
    "    return net, net_input, ksp_masked, img_masked, slice_ksp\n",
    "\n",
    "def data_consistency(img_out, slice_ksp, mask1d):\n",
    "    ''' perform data-consistency step so no \n",
    "        parameters:\n",
    "                img_out: network output image, shape torch.Size([30, x, y])\n",
    "                slice_ksp: original k-space measurements \n",
    "        returns:\n",
    "                img_dc: data-consistent output image\n",
    "                img_est: output image without data consistency '''\n",
    "    \n",
    "    img_out = reshape_complex_channels_to_sep_dimn(img_out)\n",
    "\n",
    "    # now get F*G(\\hat{C}), i.e. estimated recon in k-space\n",
    "    ksp_est = fft_2d(img_out) # ([15, 640, 368, 2])\n",
    "    ksp_orig = np_to_tt(split_complex_vals(slice_ksp)) # ([15, 640, 368, 2]); slice_ksp (15,640,368) complex\n",
    "\n",
    "    # replace estimated coeffs in k-space by original coeffs if it has been sampled\n",
    "    mask1d = torch.from_numpy(np.array(mask1d, dtype=np.uint8)) # shape: torch.Size([368]) w 41 non-zero elements\n",
    "    ksp_dc = ksp_est.clone().detach().cpu()\n",
    "    ksp_dc[:,:,mask1d==1,:] = ksp_orig[:,:,mask1d==1,:]\n",
    "\n",
    "    img_dc = recon_ksp_to_img(ksp_dc)\n",
    "    img_est = recon_ksp_to_img(ksp_est.detach().cpu())\n",
    "    \n",
    "    return img_dc, img_est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## UNCOMMENT for half precision and INJECT before fitting network\n",
    "# net = net.half()\n",
    "# ksp_masked = ksp_masked.half()\n",
    "# img_masked = img_masked.half()\n",
    "# mask = mask.half()\n",
    "# dtype=torch.cuda.HalfTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run expmts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_id 1000186 w ksp shape (num_slices, num_coils, x, y): (34, 15, 640, 372)\n",
      "file_id: 1000186\n",
      "# parameters of ConvDecoder: 1850560\n",
      "torch.Size([1, 15, 640, 372, 2]) torch.Size([1, 30, 640, 372])\n"
     ]
    }
   ],
   "source": [
    "# file_id_list = '1000411' #'1000781'\n",
    "file_id_list = ['1000186']#, '1000361', '1001524', '1000799', '1001152', '1001132']#, '1001826', '1000522']\n",
    "  \n",
    "img_dc_list, img_est_list, img_gt_list, metrics_dc = [], [], [], []\n",
    "\n",
    "# NUM_ITER = 1000  \n",
    "NUM_ITER_LIST = [3]\n",
    "\n",
    "for idx, file_id in enumerate(file_id_list):  \n",
    "    \n",
    "    # load full mri measurements\n",
    "    f, slice_ksp = load_h5(file_id)\n",
    "    if f['kspace'].shape[3] == 320:\n",
    "        continue\n",
    "    print('file_id: {}'.format(file_id))\n",
    "\n",
    "    # load mask, M\n",
    "    mask, mask2d, mask1d = get_masks(f, slice_ksp)\n",
    "    \n",
    "    # make torch versions for data consistency step in fit()\n",
    "    mask1d_ = torch.from_numpy(np.array(mask1d, dtype=np.uint8)) \n",
    "    ksp_orig = np_to_tt(split_complex_vals(slice_ksp)) # ([15, 640, 368, 2]); slice_ksp (15,640,368) complex\n",
    "    \n",
    "    for NUM_ITER in NUM_ITER_LIST:\n",
    "    \n",
    "        net, net_input, ksp_masked, img_masked, slice_ksp = \\\n",
    "                init_convdecoder(slice_ksp, mask)\n",
    "        \n",
    "        net, mse_wrt_ksp, mse_wrt_img = fit(\n",
    "            ksp_masked=ksp_masked, img_masked=img_masked,\n",
    "            net=net, net_input=net_input, mask2d=mask2d,\n",
    "            mask1d=mask1d_, ksp_orig=ksp_orig,\n",
    "            img_ls=None, num_iter=NUM_ITER, dtype=dtype)\n",
    "\n",
    "        img_out = net(net_input.type(dtype))[0] # estimate image \\hat{x} = G(\\hat{C})\n",
    "\n",
    "        img_dc, img_est = data_consistency(img_out, slice_ksp, mask1d)\n",
    "        img_gt = recon_ksp_to_img(slice_ksp) # must do this after slice_ksp is scaled\n",
    "\n",
    "        # save images, metrics\n",
    "        img_dc_list.append(img_dc)\n",
    "        img_est_list.append(img_est)\n",
    "        img_gt_list.append(img_gt) # could do this once per loop\n",
    "#     metrics_dc.append(calc_metrics(img_dc, img_gt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KEY_WORD = 'last_slice' #'iter{}'.format(NUM_ITER)\n",
    "\n",
    "for i in np.arange(0, 2*len(file_id_list), 2):\n",
    "    \n",
    "    fig = plt.figure(figsize=(16, 8))\n",
    "\n",
    "    ax1 = fig.add_subplot(141)\n",
    "    ax1.imshow(img_gt_list[i], cmap='gray')\n",
    "    ax1.set_title('ground-truth?')\n",
    "    ax1.axis('off')\n",
    "\n",
    "    ax2 = fig.add_subplot(142)\n",
    "    ax2.imshow(img_dc_list[i], cmap='gray')\n",
    "    ax2.set_title('conv_decoder, iter {}'.format(np.array(NUM_ITER_LIST).min()))\n",
    "    ax2.axis('off')\n",
    "\n",
    "    ax3 = fig.add_subplot(143)\n",
    "    ax3.imshow(img_dc_list[i+1], cmap='gray')\n",
    "    ax3.set_title('conv_decoder')\n",
    "    ax3.axis('off')\n",
    "    \n",
    "    ax4 = fig.add_subplot(144)\n",
    "    ax4.imshow(img_est_list[i+1], cmap='gray')\n",
    "    ax4.set_title('conv_decoder w/o dc post')\n",
    "    ax4.axis('off')\n",
    "    \n",
    "    plt.savefig('png_out/sample{}_{}.png'.format(i//2, KEY_WORD))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_gt_shifted = img_gt * (img_dc.mean() / img_gt.mean())\n",
    "\n",
    "# est is output image without data consistency step\n",
    "plt.hist(img_est.flatten(), bins=100, alpha=0.5, label='est')\n",
    "plt.hist(img_dc.flatten(), bins=100, alpha=0.5, label='dc')\n",
    "plt.hist(img_gt.flatten(), bins=100, alpha=0.5, label='gt')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
