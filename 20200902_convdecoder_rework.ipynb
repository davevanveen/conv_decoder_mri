{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Purpose\n",
    "\n",
    "Recreate deep decoder experiments run in `ConvDecoder_vs_DIP_vs_DD_multicoil.ipynb`, hereon referred to as the original notebook, which was extremely messy and unnecessarily complicated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import h5py\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "from utils.transform import np_to_tt, np_to_var, apply_mask, ifft_2d, fft_2d, \\\n",
    "                            reshape_complex_channels_to_sep_dimn, \\\n",
    "                            reshape_complex_channels_to_be_adj, \\\n",
    "                            split_complex_vals, combine_complex_channels, \\\n",
    "                            crop_center, root_sum_of_squares, recon_ksp_to_img\n",
    "from utils.helpers import num_params, load_h5, get_masks\n",
    "from include.decoder_conv import convdecoder\n",
    "from include.mri_helpers import get_scale_factor\n",
    "from include.fit import fit\n",
    "\n",
    "from pytorch_msssim import ms_ssim\n",
    "from utils.evaluate import calc_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.enabled = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    dtype = torch.cuda.FloatTensor\n",
    "    torch.cuda.set_device(1)\n",
    "#     print(\"num GPUs\",torch.cuda.device_count())\n",
    "else:\n",
    "    dtype = torch.FloatTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up ConvDecoder\n",
    "\n",
    "##### TODO's\n",
    "- make separate function that returns net_input given the appropriate scale_factor, i.e. split up mri_helpers.get_scale_factor() into two different functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_convdecoder(slice_ksp, mask):\n",
    "    ''' parameters: \n",
    "                slice_ksp: original, unmasked k-space measurements\n",
    "                mask: mask used to downsample original k-space\n",
    "        return:\n",
    "                net: initialized convdecoder\n",
    "                net_input: random, scaled input seed \n",
    "                ksp_masked: masked measurements to fit\n",
    "                img_masked: masked image, i.e. ifft(ksp_masked) '''\n",
    "\n",
    "    in_size = [8,4]\n",
    "    out_size = slice_ksp.shape[1:] # shape of (x,y) image slice, e.g. (640, 368)\n",
    "    out_depth = slice_ksp.shape[0]*2 # 2*n_c, i.e. 2*15=30 if multi-coil\n",
    "    num_layers = 8\n",
    "    strides = [1]*(num_layers-1)\n",
    "    num_channels = 160\n",
    "    kernel_size = 3\n",
    "\n",
    "    net = convdecoder(in_size, out_size, out_depth, num_layers, \\\n",
    "                      strides, num_channels).type(dtype)\n",
    "    print('# parameters of ConvDecoder:',num_params(net))\n",
    "    \n",
    "    # fix the scaling b/w original image and random output image = net(input tensor w values ~U[0,1]) \n",
    "    # e.g. scale_factor = 168813\n",
    "    # note: can be done using the under-sampled kspace, but we use the full kspace\n",
    "    scale_factor, net_input = get_scale_factor(net,\n",
    "                                       num_channels,\n",
    "                                       in_size,\n",
    "                                       slice_ksp)\n",
    "    slice_ksp = slice_ksp * scale_factor # original fit_untrained() f'n returns this\n",
    "\n",
    "    # mask the kspace\n",
    "    ksp_masked = apply_mask(np_to_tt(slice_ksp), mask=mask)\n",
    "    ksp_masked = np_to_var(ksp_masked.data.cpu().numpy()).type(dtype)\n",
    "\n",
    "    # perform ifft of masked kspace\n",
    "    img_masked = ifft_2d(ksp_masked[0]).cpu().numpy()\n",
    "    img_masked = reshape_complex_channels_to_be_adj(img_masked)\n",
    "    img_masked = np_to_var(img_masked).type(dtype)\n",
    "        \n",
    "    return net, net_input, ksp_masked, img_masked, slice_ksp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit network via `fit(...)`\n",
    "\n",
    "##### Returns\n",
    "- net: the best network. network output is in image space but not computed\n",
    "- mse_wrt_ksp = mse(ksp_masked, fft(out) * mask)\n",
    "- mse_wrt_img = mse(img_masked, out)\n",
    "\n",
    "##### args:\n",
    "- `ksp_masked`: masked k-space of single slice\n",
    "- `img_masked`: ifft(ksp_masked)\n",
    "- `img_ls`: least-squares recon of original (unmasked) k-space. This is used only to compute ssim, psnr, and norm_ratio across number of iterations. Too see how this is created, refer to original ipynb for definining `lsimg`\n",
    "\n",
    "Note: Original code has opt_input argument (default False) which would hence return a new version of net_input\n",
    "\n",
    "##### TODO's (in fit.py)\n",
    "- make apply_f call less confusing. compare forwardm to utils.transform.apply_mask()\n",
    "- understand why we backprop on loss_ksp and not loss_img\n",
    "- what is difference b/w \"image loss\" and \"image loss orig\"?\n",
    "\n",
    "##### Findings:\n",
    "- When evaluating on one image, 1000 iterations was sufficient. Seemingly no benefit running for 10000 images\n",
    "- Runtime per iteration is ~ 0.125s --> ~125s per 1000 iterations. Most expensive steps:\n",
    "    - backprop loss_ksp: ~0.085s\n",
    "    - compute net output: ~0.025s\n",
    "    - all the rest combined: ~0.015s\n",
    "- Cut down runtime by ~15% by removing unnecessary data type conversions\n",
    "- Can cut down runtime by ~20% using HalfTensor. In order to implement this, must do the following:\n",
    "    - Uncomment casting lines below\n",
    "    - Add apply_f(...).half() in loss_ksp calc of fit.py\n",
    "    - After all this, gradients go to zero because their values are small and cannot be represented in fp16. As such, follow the blog post under \"Mixed-Precision Training Iteration\" at https://developer.nvidia.com/blog/mixed-precision-training-deep-neural-networks/. Note: cannot use autocast, as it currently isn't supported in the main version of pytorch\n",
    "    - Once that works, need to verify no loss in output image quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## UNCOMMENT for half precision and INJECT before fitting network\n",
    "# net = net.half()\n",
    "# ksp_masked = ksp_masked.half()\n",
    "# img_masked = img_masked.half()\n",
    "# mask = mask.half()\n",
    "# dtype=torch.cuda.HalfTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(mse_wrt_ksp, label='ksp')\n",
    "# plt.plot(mse_wrt_img, label='img')\n",
    "# plt.ylim(0, 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: fix method for computing gt\n",
    "\n",
    "currently we see that mse_wrt_ksp and mse_wrt_img decrease as NUM_ITER increases. However, ssim and psnr computed with img_gt actually get worse as NUM_ITER increases. Must not be computing img_gt correctly\n",
    "\n",
    "current method: perform ifft of original k-space ksp_orig, combine complex values, combine multi-channel via rss (same method for recon of ksp_dc and ksp_est)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform data consistency step\n",
    "\n",
    "Compute network output, convert to k-space and perform data-consistency step, then convert back to image space\n",
    "\n",
    "What is actually happening in this dc step?\n",
    "- 41/368 of mask coefficients are set to true \n",
    "- 41 columns, e.g. 41 * 640 = 787200 of values in ksp are overwritten\n",
    "\n",
    "##### TODO:\n",
    "- check and reduce redundant computations, i.e. here we subsample k-space again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_consistency(img_out, slice_ksp, mask1d):\n",
    "    ''' perform data-consistency step so no \n",
    "        parameters:\n",
    "                img_out: network output image, shape torch.Size([30, x, y])\n",
    "                slice_ksp: original k-space measurements \n",
    "        returns:\n",
    "                img_dc: data-consistent output image\n",
    "                img_est: output image without data consistency '''\n",
    "    \n",
    "    img_out = reshape_complex_channels_to_sep_dimn(img_out)\n",
    "\n",
    "    # now get F*G(\\hat{C}), i.e. estimated recon in k-space\n",
    "    ksp_est = fft_2d(img_out) # ([15, 640, 368, 2])\n",
    "    ksp_orig = np_to_tt(split_complex_vals(slice_ksp)) # ([15, 640, 368, 2]); slice_ksp (15,640,368) complex\n",
    "\n",
    "    # replace estimated coeffs in k-space by original coeffs if it has been sampled\n",
    "    mask1d = torch.from_numpy(np.array(mask1d, dtype=np.uint8)) # shape: torch.Size([368]) w 41 non-zero elements\n",
    "    ksp_dc = ksp_est.clone().detach().cpu()\n",
    "    ksp_dc[:,:,mask1d==1,:] = ksp_orig[:,:,mask1d==1,:]\n",
    "\n",
    "    img_dc = recon_ksp_to_img(ksp_dc)\n",
    "    img_est = recon_ksp_to_img(ksp_est.detach().cpu())\n",
    "    \n",
    "    return img_dc, img_est"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO for new ipynb\n",
    "- call load_h5() with full path instead of file_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filename = '/bmrNAS/people/dvv/multicoil_test_v2/file{}_v2.h5'.format(file_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_id 1000186 w ksp shape (num_slices, num_coils, x, y): (34, 15, 640, 372)\n",
      "file_id: 1000186\n",
      "# parameters of ConvDecoder: 1850560\n",
      "torch.Size([1, 15, 640, 372, 2]) torch.Size([1, 30, 640, 372])\n"
     ]
    }
   ],
   "source": [
    "# file_id_list = '1000411' #'1000781'\n",
    "file_id_list = ['1000186']#, '1000361', '1001524', '1000799', '1001152', '1001132']#, '1001826', '1000522']\n",
    "  \n",
    "img_dc_list, img_est_list, img_gt_list, metrics_dc = [], [], [], []\n",
    "\n",
    "# NUM_ITER = 1000  \n",
    "NUM_ITER_LIST = [3]#, 1000]\n",
    "\n",
    "for idx, file_id in enumerate(file_id_list):  \n",
    "    \n",
    "    # load full mri measurements\n",
    "    f, slice_ksp = load_h5(file_id)\n",
    "    if f['kspace'].shape[3] == 320:\n",
    "        continue\n",
    "    print('file_id: {}'.format(file_id))\n",
    "\n",
    "    # load mask, M\n",
    "    mask, mask2d, mask1d = get_masks(f, slice_ksp)\n",
    "    \n",
    "    # make torch versions for data consistency step in fit()\n",
    "    mask1d_ = torch.from_numpy(np.array(mask1d, dtype=np.uint8)) \n",
    "    ksp_orig = np_to_tt(split_complex_vals(slice_ksp)) # ([15, 640, 368, 2]); slice_ksp (15,640,368) complex\n",
    "    \n",
    "    for NUM_ITER in NUM_ITER_LIST:\n",
    "    \n",
    "        net, net_input, ksp_masked, img_masked, slice_ksp = \\\n",
    "                init_convdecoder(slice_ksp, mask)\n",
    "        \n",
    "        net, mse_wrt_ksp, mse_wrt_img = fit(\n",
    "            ksp_masked=ksp_masked, img_masked=img_masked,\n",
    "            net=net, net_input=net_input, mask2d=mask2d,\n",
    "            mask1d=mask1d_, ksp_orig=ksp_orig,\n",
    "            img_ls=None, num_iter=NUM_ITER, dtype=dtype)\n",
    "\n",
    "        img_out = net(net_input.type(dtype))[0] # estimate image \\hat{x} = G(\\hat{C})\n",
    "\n",
    "        img_dc, img_est = data_consistency(img_out, slice_ksp, mask1d)\n",
    "        img_gt = recon_ksp_to_img(slice_ksp) # must do this after slice_ksp is scaled\n",
    "\n",
    "        # save images, metrics\n",
    "        img_dc_list.append(img_dc)\n",
    "        img_est_list.append(img_est)\n",
    "        img_gt_list.append(img_gt) # could do this once per loop\n",
    "#     metrics_dc.append(calc_metrics(img_dc, img_gt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vanveen/heck/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3425: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KEY_WORD = 'last_slice' #'iter{}'.format(NUM_ITER)\n",
    "\n",
    "for i in np.arange(0, 2*len(file_id_list), 2):\n",
    "    \n",
    "    fig = plt.figure(figsize=(16, 8))\n",
    "\n",
    "    ax1 = fig.add_subplot(141)\n",
    "    ax1.imshow(img_gt_list[i], cmap='gray')\n",
    "    ax1.set_title('ground-truth?')\n",
    "    ax1.axis('off')\n",
    "\n",
    "    ax2 = fig.add_subplot(142)\n",
    "    ax2.imshow(img_dc_list[i], cmap='gray')\n",
    "    ax2.set_title('conv_decoder, iter {}'.format(np.array(NUM_ITER_LIST).min()))\n",
    "    ax2.axis('off')\n",
    "\n",
    "    ax3 = fig.add_subplot(143)\n",
    "    ax3.imshow(img_dc_list[i+1], cmap='gray')\n",
    "    ax3.set_title('conv_decoder')\n",
    "    ax3.axis('off')\n",
    "    \n",
    "    ax4 = fig.add_subplot(144)\n",
    "    ax4.imshow(img_est_list[i+1], cmap='gray')\n",
    "    ax4.set_title('conv_decoder w/o dc post')\n",
    "    ax4.axis('off')\n",
    "    \n",
    "    plt.savefig('png_out/sample{}_{}.png'.format(i//2, KEY_WORD))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO 16 September    \n",
    "- get baseline performance over e.g. 10 images\n",
    "- implement data consistency in last layer; compare performance to the same 10 images\n",
    "- review papers sent by akshay\n",
    "\n",
    "\n",
    "\n",
    "- data consistency in the loss\n",
    "    - need to implement dc step into torch variables (done in numpy above)\n",
    "    - then i can call this similar to how apply_f()=forwardm() is done in current `fit.py`\n",
    "- next step: how to do layer-wise data consistency?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_gt_shifted = img_gt * (img_dc.mean() / img_gt.mean())\n",
    "\n",
    "# est is output image without data consistency step\n",
    "plt.hist(img_est.flatten(), bins=100, alpha=0.5, label='est')\n",
    "plt.hist(img_dc.flatten(), bins=100, alpha=0.5, label='dc')\n",
    "plt.hist(img_gt.flatten(), bins=100, alpha=0.5, label='gt')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
