{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Purpose\n",
    "\n",
    "Recreate deep decoder experiments run in `ConvDecoder_vs_DIP_vs_DD_multicoil.ipynb`, hereon referred to as the original notebook, which was extremely messy and unnecessarily complicated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import h5py\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "from utils.transform import np_to_tt, np_to_var, apply_mask, ifft_2d, fft_2d, \\\n",
    "                            reshape_complex_channels_to_sep_dimn, \\\n",
    "                            reshape_complex_channels_to_be_adj, \\\n",
    "                            split_complex_vals, combine_complex_channels, \\\n",
    "                            crop_center, root_sum_of_squares\n",
    "from utils.helpers import num_params\n",
    "from include.decoder_conv import convdecoder\n",
    "from include.mri_helpers import get_scale_factor\n",
    "from include.fit import fit\n",
    "\n",
    "from pytorch_msssim import ms_ssim\n",
    "from common.evaluate import vifp_mscale, ssim, psnr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.enabled = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    dtype = torch.cuda.FloatTensor\n",
    "    torch.cuda.set_device(0)\n",
    "#     print(\"num GPUs\",torch.cuda.device_count())\n",
    "else:\n",
    "    dtype = torch.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recon_ksp_to_img(ksp, dim=320):\n",
    "    ''' given a 3D npy array (or torch tensor) ksp k-space e.g. shape (15,x,y)\n",
    "        (1) perform ifft to img space\n",
    "        (2) reshape/combine complex channels\n",
    "        (3) combine multiple coils via root sum of squares\n",
    "        (4) crop center portion of image according to dim\n",
    "    '''\n",
    "    \n",
    "    if type(ksp).__module__ == np.__name__:\n",
    "        ksp = np_to_tt(ksp)\n",
    "\n",
    "    arr = ifft_2d(ksp).cpu().numpy()\n",
    "    arr = reshape_complex_channels_to_be_adj(arr)\n",
    "    arr = combine_complex_channels(arr) # e.g. shape (30,x,y) --> (15,x,y)\n",
    "    arr = root_sum_of_squares(arr) # e.g. (15,x,y) --> (x,y)\n",
    "    arr = crop_center(arr, dim, dim) # e.g. (x,y) --> (dim,dim)\n",
    "    \n",
    "    return arr\n",
    "\n",
    "def normalize_img(img_out, img_gt):\n",
    "    ''' normalize the pixel values in im_out according to (mean, std) of im_gt\n",
    "        TODO: decide if this step should be included\n",
    "              seems to make a big difference for vif, but not other metrics '''\n",
    "    \n",
    "    img_out = (img_out - img_out.mean()) / img_out.std()\n",
    "    img_out *= img_gt.std()\n",
    "    img_out += img_gt.mean()\n",
    "    \n",
    "    return img_out  \n",
    "\n",
    "def calc_metrics(img_out, img_gt):\n",
    "    ''' compute vif, ssim, and psnr of arr_out using im_gt as ground-truth reference '''\n",
    "    \n",
    "    img_out = normalize_img(img_out, img_gt)\n",
    "    \n",
    "    vif_ = vifp_mscale(img_out, img_gt, sigma_nsq=img_out.mean())\n",
    "    ssim_ = ssim(np.array([img_out]), np.array([img_gt]))\n",
    "    psnr_ = psnr(np.array([img_out]), np.array([img_gt]))\n",
    "    \n",
    "    dt = torch.FloatTensor\n",
    "    img_out_t = torch.from_numpy(np.array([[img_out]])).type(dt)\n",
    "    img_gt_t = torch.from_numpy(np.array([[img_gt]])).type(dt)\n",
    "    msssim_ = ms_ssim(img_out_t, img_gt_t, data_range=img_gt_t.max())\n",
    "    msssim_ = msssim_.data.cpu().numpy()[np.newaxis][0]\n",
    "    \n",
    "    return vif_, msssim_, ssim_, psnr_ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load MRI measurements, y\n",
    "\n",
    "Isolate individual 2D slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k-space shape (num_slices, num_coils, x, y):  (40, 15, 640, 368)\n"
     ]
    }
   ],
   "source": [
    "file_id = '1000411' #'1000781'\n",
    "\n",
    "filename = '/bmrNAS/people/dvv/multicoil_test_v2/file{}_v2.h5'.format(file_id)\n",
    "f = h5py.File(filename, 'r') \n",
    "# print('h5 file keys: ', f.keys())\n",
    "print('k-space shape (num_slices, num_coils, x, y): ', f['kspace'].shape)\n",
    "\n",
    "# isolate central k-space slice\n",
    "slice_idx = f['kspace'].shape[0] // 2\n",
    "slice_ksp = f['kspace'][slice_idx]\n",
    "# note: didn't add tensor version e.g. slice_ksp_torchtensor in original"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load mask, M\n",
    "\n",
    "- Format of loaded mask is 1d binary vector of size ~368, i.e. sampling of vertical lines in image\n",
    "- Convert mask to 0's and 1's, zero pad, convert to 2D, create torch transform\n",
    "\n",
    "Note: See original notebook for generating a new mask, e.g. if .h5 doesn't have a mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "under-sampling factor: 8.76\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    mask1d = np.array([1 if e else 0 for e in f[\"mask\"]]) # load 1D binary mask\n",
    "except:\n",
    "    print('Implement method for generating a mask')\n",
    "    sys.exit()\n",
    "    \n",
    "# zero out mask in outer regions e.g. mask and data have last dimn 368, but actual data is size 320\n",
    "# TODO: if actual data is size 320, then why do we have dimn 368?\n",
    "idxs_zero = (mask1d.shape[-1] - 320) // 2 # e.g. zero first/last (368-320)/2=24 indices\n",
    "mask1d[:idxs_zero], mask1d[-idxs_zero:] = 0, 0\n",
    "\n",
    "# create 2d mask. zero pad if dimensions don't line up - is this necessary?\n",
    "mask2d = np.repeat(mask1d[None,:], slice_ksp.shape[1], axis=0)#.astype(int)\n",
    "mask2d = np.pad(mask2d, ((0,),((slice_ksp.shape[-1]-mask2d.shape[-1])//2,)), mode='constant')\n",
    "\n",
    "# convert shape e.g. (368,) --> (1, 1, 368, 1)\n",
    "mask = np_to_tt(np.array([[mask2d[0][np.newaxis].T]])).type(torch.FloatTensor)\n",
    "print('under-sampling factor:', round(len(mask1d) / sum(mask1d), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up ConvDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(15, 8), (28, 15), (53, 28), (98, 53), (183, 102), (343, 193), (640, 368)]\n",
      "# parameters of ConvDecoder: 1850560\n"
     ]
    }
   ],
   "source": [
    "arch_name = 'ConvDecoder'\n",
    "\n",
    "in_size = [8,4]\n",
    "out_size = slice_ksp.shape[1:] # shape of (x,y) image slice, e.g. (640, 368)\n",
    "out_depth = slice_ksp.shape[0]*2 # 2*n_c, i.e. 2*15=30 if multi-coil\n",
    "num_layers = 8\n",
    "strides = [1]*(num_layers-1)\n",
    "num_channels = 160\n",
    "kernel_size = 3\n",
    "# dtype = \n",
    "\n",
    "net = convdecoder(in_size, out_size, out_depth, num_layers, \\\n",
    "                  strides, num_channels).type(dtype)\n",
    "\n",
    "print('# parameters of {}:'.format(arch_name),num_params(net))\n",
    "#print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit ConvDecoder\n",
    "\n",
    "##### TODO's\n",
    "- clean up unnecessary conversions b/w numpy, torch tensor [C,H,W], and torch var [1,C,H,W]\n",
    "- make separate function that returns net_input given the appropriate scale_factor, i.e. split up mri_helpers.get_scale_factor() into two different functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix the scaling b/w original image and random output image = net(input tensor w values ~U[0,1]) \n",
    "# e.g. scale_factor = 168813\n",
    "# note: can be done using the under-sampled kspace, but we use the full kspace\n",
    "scale_factor, net_input = get_scale_factor(net,\n",
    "                                   num_channels,\n",
    "                                   in_size,\n",
    "                                   slice_ksp)#,\n",
    "                                   #dtype=torch.FloatTensor)\n",
    "slice_ksp = slice_ksp * scale_factor # original fit_untrained() f'n returns this\n",
    "    \n",
    "# mask the kspace\n",
    "ksp_masked = apply_mask(np_to_tt(slice_ksp), mask=mask)\n",
    "ksp_masked = np_to_var(ksp_masked.data.cpu().numpy()).type(dtype)\n",
    "\n",
    "# perform ifft of masked kspace\n",
    "img_masked = ifft_2d(ksp_masked[0]).cpu().numpy()\n",
    "img_masked = reshape_complex_channels_to_be_adj(img_masked)\n",
    "img_masked = np_to_var(img_masked).type(dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit network via `fit(...)`\n",
    "\n",
    "##### Returns\n",
    "- net: the best network. network output is in image space but not computed\n",
    "- mse_wrt_ksp = mse(ksp_masked, fft(out) * mask)\n",
    "- mse_wrt_img = mse(img_masked, out)\n",
    "\n",
    "##### args:\n",
    "- `ksp_masked`: masked k-space of single slice\n",
    "- `img_masked`: ifft(ksp_masked)\n",
    "- `img_ls`: least-squares recon of original (unmasked) k-space. This is used only to compute ssim, psnr, and norm_ratio across number of iterations. Too see how this is created, refer to original ipynb for definining `lsimg`\n",
    "\n",
    "Note: Original code has opt_input argument (default False) which would hence return a new version of net_input\n",
    "\n",
    "##### TODO's (in fit.py)\n",
    "- make apply_f call less confusing. compare forwardm to utils.transform.apply_mask()\n",
    "- understand why we backprop on loss_ksp and not loss_img\n",
    "- what is difference b/w \"image loss\" and \"image loss orig\"?\n",
    "\n",
    "##### Findings:\n",
    "- When evaluating on one image, 1000 iterations was sufficient. Seemingly no benefit running for 10000 images\n",
    "- Runtime per iteration is ~ 0.125s --> ~125s per 1000 iterations. Most expensive steps:\n",
    "    - backprop loss_ksp: ~0.085s\n",
    "    - compute net output: ~0.025s\n",
    "    - all the rest combined: ~0.015s\n",
    "- Cut down runtime by ~15% by removing unnecessary data type conversions\n",
    "- Can cut down runtime by ~20% using HalfTensor. In order to implement this, must do the following:\n",
    "    - Uncomment casting lines below\n",
    "    - Add apply_f(...).half() in loss_ksp calc of fit.py\n",
    "    - After all this, gradients go to zero because their values are small and cannot be represented in fp16. As such, follow the blog post under \"Mixed-Precision Training Iteration\" at https://developer.nvidia.com/blog/mixed-precision-training-deep-neural-networks/. Note: cannot use autocast, as it currently isn't supported in the main version of pytorch\n",
    "    - Once that works, need to verify no loss in output image quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## UNCOMMENT for half precision\n",
    "# net = net.half()\n",
    "# ksp_masked = ksp_masked.half()\n",
    "# img_masked = img_masked.half()\n",
    "# mask = mask.half()\n",
    "# dtype=torch.cuda.HalfTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimize with adam 0.01\n",
      "Time for fit: 12.994202375411987s 0.914697  img loss 0.950774 \n"
     ]
    }
   ],
   "source": [
    "torch.cuda.synchronize()\n",
    "tt0 = time.time()\n",
    "\n",
    "net, mse_wrt_ksp, mse_wrt_img = fit(\n",
    "        ksp_masked=ksp_masked, img_masked=img_masked,\n",
    "        net=net, net_input=net_input, mask=mask2d,\n",
    "        img_ls=None, num_iter=100, dtype=dtype)\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "print('Time for fit: {}'.format(time.time()-tt0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 0.05)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(mse_wrt_ksp, label='ksp')\n",
    "plt.plot(mse_wrt_img, label='img')\n",
    "plt.ylim(0, 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform data consistency step\n",
    "\n",
    "Compute network output, convert to k-space and perform data-consistency step, then convert back to image space\n",
    "\n",
    "What is actually happening in this dc step?\n",
    "- 41/368 of mask coefficients are set to true \n",
    "- 41 columns, e.g. 41 * 640 = 787200 of values in ksp are overwritten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate image \\hat{x} = G(\\hat{C})\n",
    "img_est = net(net_input.type(dtype))[0] # shape: torch.Size([30, 640, 368])\n",
    "img_est = reshape_complex_channels_to_sep_dimn(img_est)\n",
    "    \n",
    "# now get F*G(\\hat{C}), i.e. estimated recon in k-space\n",
    "ksp_est = fft_2d(img_est) # ([15, 640, 368, 2])\n",
    "ksp_orig = np_to_tt(split_complex_vals(slice_ksp)) # ([15, 640, 368, 2]); slice_ksp (15,640,368) complex\n",
    "\n",
    "# replace estimated coeffs in k-space by original coeffs if it has been sampled\n",
    "mask = torch.from_numpy(np.array(mask1d, dtype=np.uint8)) # shape: torch.Size([368]) w 41 non-zero elements\n",
    "ksp_dc = ksp_est.clone().detach().cpu()\n",
    "ksp_dc[:,:,mask==1,:] = ksp_orig[:,:,mask==1,:]\n",
    "\n",
    "img_dc = recon_ksp_to_img(ksp_dc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create ground-truth image and image without data consistency\n",
    "\n",
    "##### TODO: verify whether current method for creating ground-truth is correct -- i think not\n",
    "but maybe the ground-truth for this sample is corrupted with an artifact?\n",
    "\n",
    "current method: perform ifft of original k-space ksp_orig, combine complex values, combine multi-channel via rss (same method for recon of ksp_dc and ksp_est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_gt = recon_ksp_to_img(slice_ksp)\n",
    "img_est = recon_ksp_to_img(ksp_est.detach().cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO 12 September    \n",
    "- try running over a set of images, for different slices, etc\n",
    "\n",
    "- think about implementing data consistency in the loss\n",
    "    - need to implement dc step into torch variables (done in numpy above)\n",
    "    - then i can call this similar to how apply_f()=forwardm() is done in current `fit.py`\n",
    "- next step: how to do layer-wise data consistency?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_metrics(img_est, img_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_metrics(img_dc, img_gt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16, 8))\n",
    "\n",
    "ax1 = fig.add_subplot(131)\n",
    "ax1.imshow(img_gt, cmap='gray')\n",
    "ax1.set_title('ground-truth image??')\n",
    "ax1.axis('off')\n",
    "\n",
    "ax2 = fig.add_subplot(132)\n",
    "ax2.imshow(img_dc, cmap='gray')\n",
    "ax2.set_title('conv_decoder')\n",
    "ax2.axis('off')\n",
    "\n",
    "ax2 = fig.add_subplot(133)\n",
    "ax2.imshow(img_est, cmap='gray')\n",
    "ax2.set_title('conv_decoder w/o dc')\n",
    "ax2.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# est is output image without data consistency step\n",
    "plt.hist(img_est.flatten(), bins=100, alpha=0.5, label='est')\n",
    "plt.hist(img_dc.flatten(), bins=100, alpha=0.5, label='dc')\n",
    "plt.hist(img_gt.flatten(), bins=100, alpha=0.5, label='gt')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
