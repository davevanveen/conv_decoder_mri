{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Purpose\n",
    "\n",
    "Recreate deep decoder experiments run in `ConvDecoder_vs_DIP_vs_DD_multicoil.ipynb`, hereon referred to as the original notebook, which was extremely messy and unnecessarily complicated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import h5py\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "from utils.transform import np_to_tt, np_to_var, apply_mask, ifft_2d, fft_2d, \\\n",
    "                            reshape_complex_channels_to_sep_dimn, \\\n",
    "                            reshape_complex_channels_to_be_adj, \\\n",
    "                            split_complex_vals, combine_complex_channels, \\\n",
    "                            crop_center, root_sum_of_squares\n",
    "from utils.helpers import num_params\n",
    "from include.decoder_conv import convdecoder\n",
    "from include.mri_helpers import get_scale_factor\n",
    "from include.fit import fit\n",
    "\n",
    "from pytorch_msssim import ms_ssim\n",
    "from common.evaluate import vifp_mscale, ssim, psnr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.enabled = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    dtype = torch.cuda.FloatTensor\n",
    "    torch.cuda.set_device(0)\n",
    "#     print(\"num GPUs\",torch.cuda.device_count())\n",
    "else:\n",
    "    dtype = torch.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recon_ksp_to_img(ksp, dim=320):\n",
    "    ''' given a 3D npy array (or torch tensor) ksp k-space e.g. shape (15,x,y)\n",
    "        (1) perform ifft to img space\n",
    "        (2) reshape/combine complex channels\n",
    "        (3) combine multiple coils via root sum of squares\n",
    "        (4) crop center portion of image according to dim\n",
    "    '''\n",
    "    \n",
    "    if type(ksp).__module__ == np.__name__:\n",
    "        ksp = np_to_tt(ksp)\n",
    "\n",
    "    arr = ifft_2d(ksp).cpu().numpy()\n",
    "    arr = reshape_complex_channels_to_be_adj(arr)\n",
    "    arr = combine_complex_channels(arr) # e.g. shape (30,x,y) --> (15,x,y)\n",
    "    arr = root_sum_of_squares(arr) # e.g. (15,x,y) --> (x,y)\n",
    "    arr = crop_center(arr, dim, dim) # e.g. (x,y) --> (dim,dim)\n",
    "    \n",
    "    return arr\n",
    "\n",
    "def normalize_img(img_out, img_gt):\n",
    "    ''' normalize the pixel values in im_gt according to (mean, std) of im_out\n",
    "        verified: step is necessary '''\n",
    "    \n",
    "    if img_out.mean() < img_gt.mean():\n",
    "        raise NotImplementedError('assumes img_gt has much smaller pixel vals')\n",
    "\n",
    "    img_gt = (img_gt - img_gt.mean()) / img_gt.std()\n",
    "    img_gt *= img_out.std()\n",
    "    img_gt += img_out.mean()\n",
    "    \n",
    "    return img_gt\n",
    "\n",
    "def calc_metrics(img_out, img_gt):\n",
    "    ''' compute vif, ssim, and psnr of arr_out using im_gt as ground-truth reference '''\n",
    "    \n",
    "    img_gt = normalize_img(img_out, img_gt)\n",
    "    \n",
    "    vif_ = vifp_mscale(img_out, img_gt, sigma_nsq=img_out.mean())\n",
    "    ssim_ = ssim(np.array([img_out]), np.array([img_gt]))\n",
    "    psnr_ = psnr(np.array([img_out]), np.array([img_gt]))\n",
    "    \n",
    "    dt = torch.FloatTensor\n",
    "    img_out_t = torch.from_numpy(np.array([[img_out]])).type(dt)\n",
    "    img_gt_t = torch.from_numpy(np.array([[img_gt]])).type(dt)\n",
    "    msssim_ = ms_ssim(img_out_t, img_gt_t, data_range=img_gt_t.max())\n",
    "    msssim_ = msssim_.data.cpu().numpy()[np.newaxis][0]\n",
    "    \n",
    "    return vif_, msssim_, ssim_, psnr_ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load MRI measurements, y\n",
    "\n",
    "Isolate individual 2D slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_h5(file_id):\n",
    "    ''' given file_id, return the h5 file and central slice '''\n",
    "    \n",
    "    filename = '/bmrNAS/people/dvv/multicoil_test_v2/file{}_v2.h5'.format(file_id)\n",
    "    f = h5py.File(filename, 'r') \n",
    "    \n",
    "#     print('file_id {} w ksp shape (num_slices, num_coils, x, y): {}'.format( \\\n",
    "#                                             file_id, f['kspace'].shape))\n",
    "\n",
    "    # isolate central k-space slice\n",
    "    slice_idx = f['kspace'].shape[0] // 2\n",
    "    slice_ksp = f['kspace'][slice_idx]\n",
    "    \n",
    "    return f, slice_ksp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load mask, M\n",
    "\n",
    "- Format of loaded mask is 1d binary vector of size ~368, i.e. sampling of vertical lines in image\n",
    "- Convert mask to 0's and 1's, zero pad, convert to 2D, create torch transform\n",
    "\n",
    "##### TODO's\n",
    "- work through this on low-level to simplify steps + reduce number of returned items\n",
    "\n",
    "Note: See original notebook for generating a new mask, e.g. if .h5 doesn't have a mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_masks(file_h5):\n",
    "    ''' given h5 file, return three different versions of masks:\n",
    "            mask: used for masking k-space as network input\n",
    "            mask2d: 2D mask used to fit network \n",
    "            mask1d: 1D mask used for data consistency step '''\n",
    "    try:\n",
    "        mask1d = np.array([1 if e else 0 for e in file_h5[\"mask\"]]) # load 1D binary mask\n",
    "    except:\n",
    "        print('Implement method for generating a mask')\n",
    "        sys.exit()\n",
    "\n",
    "    # zero out mask in outer regions e.g. mask and data have last dimn 368, but actual data is size 320\n",
    "    # TODO: if actual data is size 320, then why do we have dimn 368?\n",
    "    idxs_zero = (mask1d.shape[-1] - 320) // 2 # e.g. zero first/last (368-320)/2=24 indices\n",
    "    mask1d[:idxs_zero], mask1d[-idxs_zero:] = 0, 0\n",
    "\n",
    "    # create 2d mask. zero pad if dimensions don't line up - is this necessary?\n",
    "    mask2d = np.repeat(mask1d[None,:], slice_ksp.shape[1], axis=0)#.astype(int)\n",
    "    mask2d = np.pad(mask2d, ((0,),((slice_ksp.shape[-1]-mask2d.shape[-1])//2,)), mode='constant')\n",
    "\n",
    "    # convert shape e.g. (368,) --> (1, 1, 368, 1)\n",
    "    mask = np_to_tt(np.array([[mask2d[0][np.newaxis].T]])).type(torch.FloatTensor)\n",
    "    #print('under-sampling factor:', round(len(mask1d) / sum(mask1d), 2))\n",
    "    \n",
    "    return mask, mask2d, mask1d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up ConvDecoder\n",
    "\n",
    "##### TODO's\n",
    "- make separate function that returns net_input given the appropriate scale_factor, i.e. split up mri_helpers.get_scale_factor() into two different functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_convdecoder(slice_ksp, mask):\n",
    "    ''' parameters: \n",
    "                slice_ksp: original, unmasked k-space measurements\n",
    "                mask: mask used to downsample original k-space\n",
    "        return:\n",
    "                net: initialized convdecoder\n",
    "                net_input: random, scaled input seed \n",
    "                ksp_masked: masked measurements to fit\n",
    "                img_masked: masked image, i.e. ifft(ksp_masked) '''\n",
    "\n",
    "    in_size = [8,4]\n",
    "    out_size = slice_ksp.shape[1:] # shape of (x,y) image slice, e.g. (640, 368)\n",
    "    out_depth = slice_ksp.shape[0]*2 # 2*n_c, i.e. 2*15=30 if multi-coil\n",
    "    num_layers = 8\n",
    "    strides = [1]*(num_layers-1)\n",
    "    num_channels = 160\n",
    "    kernel_size = 3\n",
    "\n",
    "    net = convdecoder(in_size, out_size, out_depth, num_layers, \\\n",
    "                      strides, num_channels).type(dtype)\n",
    "    #print('# parameters of ConvDecoder:',num_params(net))\n",
    "    \n",
    "    # fix the scaling b/w original image and random output image = net(input tensor w values ~U[0,1]) \n",
    "    # e.g. scale_factor = 168813\n",
    "    # note: can be done using the under-sampled kspace, but we use the full kspace\n",
    "    scale_factor, net_input = get_scale_factor(net,\n",
    "                                       num_channels,\n",
    "                                       in_size,\n",
    "                                       slice_ksp)\n",
    "    slice_ksp = slice_ksp * scale_factor # original fit_untrained() f'n returns this\n",
    "\n",
    "    # mask the kspace\n",
    "    ksp_masked = apply_mask(np_to_tt(slice_ksp), mask=mask)\n",
    "    ksp_masked = np_to_var(ksp_masked.data.cpu().numpy()).type(dtype)\n",
    "\n",
    "    # perform ifft of masked kspace\n",
    "    img_masked = ifft_2d(ksp_masked[0]).cpu().numpy()\n",
    "    img_masked = reshape_complex_channels_to_be_adj(img_masked)\n",
    "    img_masked = np_to_var(img_masked).type(dtype)\n",
    "    \n",
    "    return net, net_input, ksp_masked, img_masked, slice_ksp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit network via `fit(...)`\n",
    "\n",
    "##### Returns\n",
    "- net: the best network. network output is in image space but not computed\n",
    "- mse_wrt_ksp = mse(ksp_masked, fft(out) * mask)\n",
    "- mse_wrt_img = mse(img_masked, out)\n",
    "\n",
    "##### args:\n",
    "- `ksp_masked`: masked k-space of single slice\n",
    "- `img_masked`: ifft(ksp_masked)\n",
    "- `img_ls`: least-squares recon of original (unmasked) k-space. This is used only to compute ssim, psnr, and norm_ratio across number of iterations. Too see how this is created, refer to original ipynb for definining `lsimg`\n",
    "\n",
    "Note: Original code has opt_input argument (default False) which would hence return a new version of net_input\n",
    "\n",
    "##### TODO's (in fit.py)\n",
    "- make apply_f call less confusing. compare forwardm to utils.transform.apply_mask()\n",
    "- understand why we backprop on loss_ksp and not loss_img\n",
    "- what is difference b/w \"image loss\" and \"image loss orig\"?\n",
    "\n",
    "##### Findings:\n",
    "- When evaluating on one image, 1000 iterations was sufficient. Seemingly no benefit running for 10000 images\n",
    "- Runtime per iteration is ~ 0.125s --> ~125s per 1000 iterations. Most expensive steps:\n",
    "    - backprop loss_ksp: ~0.085s\n",
    "    - compute net output: ~0.025s\n",
    "    - all the rest combined: ~0.015s\n",
    "- Cut down runtime by ~15% by removing unnecessary data type conversions\n",
    "- Can cut down runtime by ~20% using HalfTensor. In order to implement this, must do the following:\n",
    "    - Uncomment casting lines below\n",
    "    - Add apply_f(...).half() in loss_ksp calc of fit.py\n",
    "    - After all this, gradients go to zero because their values are small and cannot be represented in fp16. As such, follow the blog post under \"Mixed-Precision Training Iteration\" at https://developer.nvidia.com/blog/mixed-precision-training-deep-neural-networks/. Note: cannot use autocast, as it currently isn't supported in the main version of pytorch\n",
    "    - Once that works, need to verify no loss in output image quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## UNCOMMENT for half precision and INJECT before fitting network\n",
    "# net = net.half()\n",
    "# ksp_masked = ksp_masked.half()\n",
    "# img_masked = img_masked.half()\n",
    "# mask = mask.half()\n",
    "# dtype=torch.cuda.HalfTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(mse_wrt_ksp, label='ksp')\n",
    "# plt.plot(mse_wrt_img, label='img')\n",
    "# plt.ylim(0, 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: fix method for computing gt\n",
    "\n",
    "currently we see that mse_wrt_ksp and mse_wrt_img decrease as NUM_ITER increases. However, ssim and psnr computed with img_gt actually get worse as NUM_ITER increases. Must not be computing img_gt correctly\n",
    "\n",
    "current method: perform ifft of original k-space ksp_orig, combine complex values, combine multi-channel via rss (same method for recon of ksp_dc and ksp_est)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform data consistency step\n",
    "\n",
    "Compute network output, convert to k-space and perform data-consistency step, then convert back to image space\n",
    "\n",
    "What is actually happening in this dc step?\n",
    "- 41/368 of mask coefficients are set to true \n",
    "- 41 columns, e.g. 41 * 640 = 787200 of values in ksp are overwritten\n",
    "\n",
    "##### TODO:\n",
    "- check and reduce redundant computations, i.e. here we subsample k-space again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_consistency(img_out, slice_ksp, mask1d):\n",
    "    ''' perform data-consistency step so no \n",
    "        parameters:\n",
    "                img_out: network output image, shape torch.Size([30, x, y])\n",
    "                slice_ksp: original k-space measurements \n",
    "        returns:\n",
    "                img_dc: data-consistent output image\n",
    "                img_est: output image without data consistency '''\n",
    "    \n",
    "    img_out = reshape_complex_channels_to_sep_dimn(img_out)\n",
    "\n",
    "    # now get F*G(\\hat{C}), i.e. estimated recon in k-space\n",
    "    ksp_est = fft_2d(img_out) # ([15, 640, 368, 2])\n",
    "    ksp_orig = np_to_tt(split_complex_vals(slice_ksp)) # ([15, 640, 368, 2]); slice_ksp (15,640,368) complex\n",
    "\n",
    "    # replace estimated coeffs in k-space by original coeffs if it has been sampled\n",
    "    mask1d = torch.from_numpy(np.array(mask1d, dtype=np.uint8)) # shape: torch.Size([368]) w 41 non-zero elements\n",
    "    ksp_dc = ksp_est.clone().detach().cpu()\n",
    "    ksp_dc[:,:,mask1d==1,:] = ksp_orig[:,:,mask1d==1,:]\n",
    "\n",
    "    img_dc = recon_ksp_to_img(ksp_dc)\n",
    "    img_est = recon_ksp_to_img(ksp_est.detach().cpu())\n",
    "    \n",
    "    return img_dc, img_est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_id: 1000186\n",
      "file_id: 1000361\n"
     ]
    }
   ],
   "source": [
    "# file_id_list = '1000411' #'1000781'\n",
    "file_id_list = ['1000186', '1000361', '1001524', '1000799', '1001152', '1001132']#, '1001826', '1000522']\n",
    "  \n",
    "img_dc_list, img_est_list, img_gt_list, metrics_dc = [], [], [], []\n",
    "\n",
    "# NUM_ITER = 1000  \n",
    "NUM_ITER_LIST = [1, 1000]\n",
    "\n",
    "for idx, file_id in enumerate(file_id_list):  \n",
    "    \n",
    "    f, slice_ksp = load_h5(file_id)\n",
    "    if f['kspace'].shape[3] == 320:\n",
    "        continue\n",
    "    print('file_id: {}'.format(file_id))\n",
    "\n",
    "    mask, mask2d, mask1d = get_masks(f)\n",
    "\n",
    "    for NUM_ITER in NUM_ITER_LIST:\n",
    "    \n",
    "        net, net_input, ksp_masked, img_masked, slice_ksp = \\\n",
    "                init_convdecoder(slice_ksp, mask)\n",
    "\n",
    "        net, mse_wrt_ksp, mse_wrt_img = fit(\n",
    "            ksp_masked=ksp_masked, img_masked=img_masked,\n",
    "            net=net, net_input=net_input, mask=mask2d,\n",
    "            img_ls=None, num_iter=NUM_ITER, dtype=dtype)\n",
    "\n",
    "        img_out = net(net_input.type(dtype))[0] # estimate image \\hat{x} = G(\\hat{C})\n",
    "\n",
    "        img_dc, img_est = data_consistency(img_out, slice_ksp, mask1d)\n",
    "        img_gt = recon_ksp_to_img(slice_ksp) # must do this after slice_ksp is scaled\n",
    "\n",
    "        img_dc_list.append(img_dc)\n",
    "        if NUM_ITER == np.array(NUM_ITER_LIST).min():\n",
    "            img_est_list.append(img_est)\n",
    "            img_gt_list.append(img_gt)\n",
    "#     metrics_dc.append(calc_metrics(img_dc, img_gt))\n",
    "        \n",
    "# img_dc_list = np.array(img_dc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KEY_WORD = '' #'iter{}'.format(NUM_ITER)\n",
    "\n",
    "for i in np.arange(len(file_id_list)):\n",
    "    \n",
    "    fig = plt.figure(figsize=(16, 8))\n",
    "\n",
    "    ax1 = fig.add_subplot(141)\n",
    "    ax1.imshow(img_gt_list[i], cmap='gray')\n",
    "    ax1.set_title('ground-truth?')\n",
    "    ax1.axis('off')\n",
    "\n",
    "    ax2 = fig.add_subplot(142)\n",
    "    ax2.imshow(img_dc_list[i], cmap='gray')\n",
    "    ax2.set_title('conv_decoder, iter {}'.format(np.array(NUM_ITER_LIST).min()))\n",
    "    ax2.axis('off')\n",
    "\n",
    "    ax3 = fig.add_subplot(143)\n",
    "    ax3.imshow(img_dc_list[i+1], cmap='gray')\n",
    "    ax3.set_title('conv_decoder')\n",
    "    ax3.axis('off')\n",
    "    \n",
    "    ax4 = fig.add_subplot(144)\n",
    "    ax4.imshow(img_est_list[i], cmap='gray')\n",
    "    ax4.set_title('conv_decoder w/o dc post')\n",
    "    ax4.axis('off')\n",
    "    \n",
    "    plt.savefig('png_out/sample{}_{}.png'.format(i, KEY_WORD))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO 16 September    \n",
    "- get baseline performance over e.g. 10 images\n",
    "- implement data consistency in last layer; compare performance to the same 10 images\n",
    "- review papers sent by akshay\n",
    "\n",
    "\n",
    "\n",
    "- data consistency in the loss\n",
    "    - need to implement dc step into torch variables (done in numpy above)\n",
    "    - then i can call this similar to how apply_f()=forwardm() is done in current `fit.py`\n",
    "- next step: how to do layer-wise data consistency?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img_gt_shifted = img_gt * (img_dc.mean() / img_gt.mean())\n",
    "\n",
    "# est is output image without data consistency step\n",
    "plt.hist(img_est.flatten(), bins=100, alpha=0.5, label='est')\n",
    "plt.hist(img_dc.flatten(), bins=100, alpha=0.5, label='dc')\n",
    "plt.hist(img_gt.flatten(), bins=100, alpha=0.5, label='gt')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
