{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Purpose\n",
    "\n",
    "Recreate deep decoder experiments run in `ConvDecoder_vs_DIP_vs_DD_multicoil.ipynb`, hereon referred to as the original notebook, which was extremely messy and unnecessarily complicated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import h5py\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils.transform import np_to_tt, np_to_var, apply_mask, ifft_2d\n",
    "from utils.helpers import num_params\n",
    "from include.decoder_conv import convdecoder\n",
    "from include.mri_helpers import get_scale_factor\n",
    "from include.helpers import np_to_var\n",
    "from include.fit import fit\n",
    "\n",
    "# TODO: fix these imports\n",
    "# from include import * \n",
    "# from include import transforms as transform\n",
    "# from common.evaluate import * \n",
    "# from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.enabled = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    dtype = torch.cuda.FloatTensor\n",
    "    torch.cuda.set_device(0)\n",
    "#     print(\"num GPUs\",torch.cuda.device_count())\n",
    "else:\n",
    "    dtype = torch.FloatTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load MRI measurements, y\n",
    "\n",
    "Isolate individual 2D slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k-space shape (num_slices, num_coils, x, y):  (37, 15, 640, 368)\n"
     ]
    }
   ],
   "source": [
    "filename = '/bmrNAS/people/dvv/multicoil_test_v2/file1000781_v2.h5'\n",
    "f = h5py.File(filename, 'r') \n",
    "# print('h5 file keys: ', f.keys())\n",
    "print('k-space shape (num_slices, num_coils, x, y): ', f['kspace'].shape)\n",
    "\n",
    "# isolate central k-space slice\n",
    "slice_idx = f['kspace'].shape[0] // 2\n",
    "slice_ksp = f['kspace'][slice_idx]\n",
    "# note: didn't add tensor version e.g. slice_ksp_torchtensor in original"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load mask, M\n",
    "\n",
    "- Format of loaded mask is 1d binary vector of size ~368, i.e. sampling of vertical lines in image\n",
    "- Convert mask to 0's and 1's, zero pad, convert to 2D, create torch transform\n",
    "\n",
    "Note: See original notebook for generating a new mask, e.g. if .h5 doesn't have a mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "under-sampling factor: 8.98\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    mask1d = np.array([1 if e else 0 for e in f[\"mask\"]]) # load 1D binary mask\n",
    "except:\n",
    "    print('Implement method for generating a mask')\n",
    "    sys.exit()\n",
    "    \n",
    "# zero out mask in outer regions e.g. mask and data have last dimn 368, but actual data is size 320\n",
    "# TODO: if actual data is size 320, then why do we have dimn 368?\n",
    "idxs_zero = (mask1d.shape[-1] - 320) // 2 # e.g. zero first/last (368-320)/2=24 indices\n",
    "mask1d[:idxs_zero], mask1d[-idxs_zero:] = 0, 0\n",
    "\n",
    "# create 2d mask. zero pad if dimensions don't line up - is this necessary?\n",
    "mask2d = np.repeat(mask1d[None,:], slice_ksp.shape[1], axis=0)#.astype(int)\n",
    "mask2d = np.pad(mask2d, ((0,),((slice_ksp.shape[-1]-mask2d.shape[-1])//2,)), mode='constant')\n",
    "\n",
    "# convert shape e.g. (368,) --> (1, 1, 368, 1)\n",
    "mask = np_to_tt(np.array([[mask2d[0][np.newaxis].T]])).type(torch.FloatTensor)\n",
    "print('under-sampling factor:', round(len(mask1d) / sum(mask1d), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up ConvDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(15, 8), (28, 15), (53, 28), (98, 53), (183, 102), (343, 193), (640, 368)]\n",
      "# parameters of ConvDecoder: 1850560\n"
     ]
    }
   ],
   "source": [
    "arch_name = 'ConvDecoder'\n",
    "\n",
    "in_size = [8,4]\n",
    "out_size = slice_ksp.shape[1:] # shape of (x,y) image slice, e.g. (640, 368)\n",
    "out_depth = slice_ksp.shape[0]*2 # 2*n_c, i.e. 2*15=30 if multi-coil\n",
    "num_layers = 8\n",
    "strides = [1]*(num_layers-1)\n",
    "num_channels = 160\n",
    "kernel_size = 3\n",
    "\n",
    "net = convdecoder(in_size, out_size, out_depth, num_layers, strides, num_channels).type(dtype)\n",
    "\n",
    "print('# parameters of {}:'.format(arch_name),num_params(net))\n",
    "#print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit ConvDecoder\n",
    "\n",
    "##### TODO's\n",
    "- clean up unnecessary conversions b/w numpy, torch tensor [C,H,W], and torch var [1,C,H,W]\n",
    "- make separate function that returns net_input given the appropriate scale_factor, i.e. split up mri_helpers.get_scale_factor() into two different functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix the scaling b/w original image and random output image = net(input tensor w values ~U[0,1]) \n",
    "# e.g. scale_factor = 168813\n",
    "# note: can be done using the under-sampled kspace, but we use the full kspace\n",
    "scale_factor, net_input = get_scale_factor(net,\n",
    "                                   num_channels,\n",
    "                                   in_size,\n",
    "                                   slice_ksp)\n",
    "slice_ksp = slice_ksp * scale_factor # original fit_untrained() f'n returns this\n",
    "slice_ksp_tt = np_to_tt(slice_ksp)\n",
    "    \n",
    "# mask the kspace\n",
    "ksp_masked_tt = apply_mask(slice_ksp_tt, mask=mask)\n",
    "# convert to torch variable [C, W, H] --> [1, C, W, H]\n",
    "ksp_masked_tt = np_to_var(ksp_masked_tt.data.cpu().numpy()).type(dtype)\n",
    "\n",
    "# perform ifft of masked kspace\n",
    "img_masked = ifft_2d(ksp_masked_tt[0]).cpu().numpy()\n",
    "# reshape complex channels to be adjacent: (15,x,y,2) --> (1,30,x,y)\n",
    "out = []\n",
    "for img in img_masked:\n",
    "    out += [img[:,:,0], img[:,:,1]]\n",
    "img_masked = np_to_var(np.array(out)).type(dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit network via `fit(...)`\n",
    "\n",
    "##### Returns\n",
    "- net: the best network. network output is in image space but not computed\n",
    "- mse_wrt_ksp = mse(ksp_masked, fft(out) * mask)\n",
    "- mse_wrt_img = mse(img_masked, out)\n",
    "\n",
    "##### args:\n",
    "- `ksp_masked`: ksp_masked_tt, i.e. masked k-space of single slice\n",
    "- `img_masked`: ifft(ksp_masked)\n",
    "- `img_ls`: least-squares recon of original (unmasked) k-space. This is used only to compute ssim, psnr, and norm_ratio across number of iterations. Too see how this is created, refer to original ipynb for definining `lsimg`\n",
    "- `find_best`: whether or not to save best network at each iteration. If set to False, I think `fit()` would just return as output the original net? Not sure why you'd ever want to do that...\n",
    "\n",
    "Note: Original code has opt_input argument (default False) which would hence return a new version of net_input\n",
    "\n",
    "##### TODO's (in fit.py)\n",
    "- make apply_f call less confusing. compare forwardm to utils.transform.apply_mask()\n",
    "- understand why we backprop on loss_ksp and not loss_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimize with adam 0.01\n",
      "Iteration 00000  kspace (training) loss 0.568088  image loss 0.683810 image loss orig (?) 0.683810 \r"
     ]
    }
   ],
   "source": [
    "_, _, _, mse_wrt_ksp, mse_wrt_img, _, net = fit(\n",
    "        ksp_masked=ksp_masked_tt, img_masked=img_masked,\n",
    "        net=net, net_input=net_input, mask=mask2d,\n",
    "        img_ls=None, num_iter=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: start here\n",
    "\n",
    "### Perform data consistency step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
