{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Purpose\n",
    "\n",
    "Previously was backpropagating on k-space using mse as loss function. This places very high emphasis on low frequency components.\n",
    "\n",
    "--> How would IQ vary if we applied transforms to emphasize high-frequency components of k-space?\n",
    "\n",
    "##### Need some non-linear transform to scale values\n",
    "\n",
    "##### Scale pixel/k-space values? No\n",
    "- Current scaling method:\n",
    "    - create net_input ~U[0,1]\n",
    "    - given randomly generated net, compute out_img\n",
    "    - given slice_ksp, compute orig_img - i.e. ifft of very small k-space vals\n",
    "    - scale_factor = norm(out_img) / norm(orig_img)\n",
    "- FastMRI k-space data magnitude values are very small (pre-scale), w a handful of largest outlier values at 1e-3, but most, i.e. 99.9% are < 1e-5\n",
    "- Conclusion: Scaling of values in k-space or image space shouldn't matter, i.e. 16-bit floating point precision can handle it regardless. Only want output values to be in same range for computing ssim, psnr, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import h5py\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "from utils.transform import np_to_tt, split_complex_vals, recon_ksp_to_img, \\\n",
    "                            reshape_complex_channels_to_be_adj, \\\n",
    "                            combine_complex_channels\n",
    "from utils.helpers import num_params, load_h5, get_masks\n",
    "from include.decoder_conv import init_convdecoder\n",
    "from include.mri_helpers import get_scale_factor, get_masked_measurements, \\\n",
    "                                data_consistency, forwardm\n",
    "from include.fit import fit\n",
    "from utils.evaluate import calc_metrics\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.enabled = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    dtype = torch.cuda.FloatTensor\n",
    "    torch.cuda.set_device(3)\n",
    "else:\n",
    "    dtype = torch.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15, 640, 368)\n",
      "(15, 640, 368)\n",
      "(15, 640, 368)\n"
     ]
    }
   ],
   "source": [
    "img_dc_list, img_est_list, img_gt_list, metrics_dc = [], [], [], []\n",
    "mse_wrt_ksp_list, mse_wrt_img_list = [], []\n",
    "ssim_list, psnr_list = [], []\n",
    "\n",
    "file_id_list = ['1000464', '1000273', '1000007']#, '1000325']#, '1000537', '1000818', \\\n",
    "                #'1001140', '1001219', '1001338', '1001598', '1001533', '1001798']\n",
    "NUM_ITER = 1\n",
    "DC_STEP = False\n",
    "\n",
    "ni_list = []\n",
    "\n",
    "for idx, file_id in enumerate(file_id_list):  \n",
    "\n",
    "    f, slice_ksp = load_h5(file_id) # load full mri measurements\n",
    "    print(slice_ksp.shape)\n",
    "    mask, mask2d, mask1d = get_masks(f, slice_ksp) # load mask + variants, M\n",
    "    mask1d_ = torch.from_numpy(np.array(mask1d, dtype=np.uint8)) # for dc step\n",
    "\n",
    "    # initialize net, net input seed, and scale slice_ksp accordingly\n",
    "    net, net_input, slice_ksp = init_convdecoder(slice_ksp, mask)\n",
    "    ni_list.append(net)\n",
    "    continue\n",
    "    \n",
    "    # for dc step - must do this after scaling slice_ksp\n",
    "    ksp_orig = np_to_tt(split_complex_vals(slice_ksp))[None, :].type(dtype) #[1,15,640,368,2]\n",
    "\n",
    "    # apply mask to measurements for fitting model\n",
    "    ksp_masked, img_masked = get_masked_measurements(slice_ksp, mask)\n",
    "\n",
    "    net, mse_wrt_ksp, mse_wrt_img = fit(\n",
    "        ksp_masked=ksp_masked, img_masked=img_masked,\n",
    "        net=net, net_input=net_input, mask2d=mask2d,\n",
    "        mask1d=mask1d_, ksp_orig=ksp_orig, DC_STEP=DC_STEP, alpha=0,#ALPHA,\n",
    "        img_ls=None, num_iter=NUM_ITER, dtype=dtype)\n",
    "\n",
    "    img_out = net(net_input.type(dtype))#[0] # estimate image \\hat{x} = G(\\hat{C})\n",
    "\n",
    "    out_ksp_masked = forwardm(img_out, mask2d)\n",
    "\n",
    "    img_dc, img_est = data_consistency(img_out[0], slice_ksp, mask1d)\n",
    "    img_gt = recon_ksp_to_img(slice_ksp) # must do this after slice_ksp is scaled\n",
    "\n",
    "    _, _, ssim_, psnr_ = calc_metrics(img_dc, img_gt)\n",
    "\n",
    "    # save images, metrics\n",
    "    img_dc_list.append(img_dc)\n",
    "    img_gt_list.append(img_gt) # could do this once per loop\n",
    "    mse_wrt_ksp_list.append(mse_wrt_ksp)\n",
    "    mse_wrt_img_list.append(mse_wrt_img)\n",
    "    ssim_list.append(ssim_)\n",
    "    psnr_list.append(psnr_)\n",
    "    \n",
    "#     ksp_masked_flat = ksp_masked.detach().cpu().numpy().flatten()\n",
    "#     ksp_orig_flat = ksp_orig.detach().cpu().numpy().flatten()\n",
    "#     val = 0.5\n",
    "#     kmf = chop_tail(ksp_masked_flat, -val, val)\n",
    "#     kof = chop_tail(ksp_orig_flat, -val, val)\n",
    "#     print(len(kmf) / len(ksp_masked_flat), len(kof) / len(ksp_orig_flat))\n",
    "\n",
    "    arr_km = combine_complex_channels(reshape_complex_channels_to_be_adj(ksp_masked[0].detach().cpu().numpy())).flatten()\n",
    "#     io = img_out[0].detach().cpu().numpy().flatten()\n",
    "#     io = combine_complex_channels(io)\n",
    "#     arr_km = io\n",
    "    ll = len(arr_km)\n",
    "    (n, bins, patches) = plt.hist(arr_km, bins=100)\n",
    "    plt.close()\n",
    "\n",
    "    arr_km = chop_tail(arr_km, bins[0], bins[1])\n",
    "    print(n[0] / ll)\n",
    "    plt.hist(arr_km, bins=50)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chop_tail(arr, val_min, val_max):\n",
    "    ''' given flat arr, remove elements not b/w val_min and val_max '''\n",
    "    arr = arr.flatten()\n",
    "    arr = arr[arr>val_min]\n",
    "    return arr[arr<val_max]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7005380\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWoAAAEDCAYAAAAcI05xAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAAAL7UlEQVR4nO3df6jddR3H8derzSwyqNjJxFk3YhVmqXWxKImyXysjKzCMCiNpBBUG/UDzj+iPwAiiCP1jlGRkilHWKEytjGXk8s6s9qNs2KKFtasmKkG1fPXH93vddZ7tfpfne877nvN8wNi55/vd8X0/bs9973ff77lOIgBAXU+Y9AAAgCMj1ABQHKEGgOIINQAUR6gBoDhCDQDF9RZq21fY3m97R8f932V7l+2dtr/V11wAsNq4r+uobb9a0kOSvpHklBX23SDpWklnJfmH7Wcm2d/LYACwyvR2RJ1kq6T7lj9n+3m2f2R7u+2f235hu+mDki5L8o/21xJpAGiN+xz1ZkkfTfIySZ+QdHn7/PMlPd/2L2zfanvjmOcCgLLWjus/ZPs4Sa+U9G3bS08fu2yODZJeI2m9pK22X5zk/nHNBwBVjS3Uao7e709y2pBt+yRtS/IfSX+yfaeacN82xvkAoKSxnfpI8oCaCJ8rSW6c2m7+npqjadlep+ZUyF3jmg0AKuvz8ryrJf1S0gts77N9gaT3SLrA9m8k7ZR0Trv7DZLutb1L0s2SPpnk3r5mA4DVpLfL8wAAo8GdiQBQXC//mLhu3brMzc318dIAMJW2b99+T5LBsG29hHpubk4LCwt9vDQATCXbfz7cNk59AEBxhBoAiiPUAFAcoQaA4gg1ABRHqAGgOEINAMURagAojlADQHHjfD9qYGzmLvrhUe2/99Kze5oEePw4ogaA4gg1ABRHqAGgOEINAMURagAojlADQHGEGgCKI9QAUByhBoDiCDUAFEeoAaA4Qg0AxRFqACiOUANAcYQaAIoj1ABQXKdvHGB7r6QHJf1X0oEk830OBQA46Gi+w8trk9zT2yQAgKE49QEAxXUNdSTdaHu77U3DdrC9yfaC7YXFxcXRTQgAM65rqM9M8lJJb5b0YduvPnSHJJuTzCeZHwwGIx0SAGZZp1An+Wv7835J10k6o8+hAAAHrRhq20+x/dSlx5LeKGlH34MBABpdrvo4XtJ1tpf2/1aSH/U6FQDgESuGOsldkk4dwywAgCG4PA8AiiPUAFAcoQaA4gg1ABRHqAGgOEINAMURagAojlADQHGEGgCKI9QAUByhBoDiCDUAFEeoAaA4Qg0AxRFqACiOUANAcYQaAIoj1ABQHKEGgOIINQAUR6gBoDhCDQDFEWoAKI5QA0BxhBoAiiPUAFAcoQaA4gg1ABTXOdS219j+te0f9DkQAODRjuaI+kJJu/saBAAwXKdQ214v6WxJX+13HADAoboeUX9J0qckPXy4HWxvsr1ge2FxcXEUswEA1CHUtt8qaX+S7UfaL8nmJPNJ5geDwcgGBIBZ1+WI+lWS3mZ7r6RrJJ1l+5u9TgUAeMSKoU5ycZL1SeYknSfpp0ne2/tkAABJXEcNAOWtPZqdk/xM0s96mQQAMBRH1ABQHKEGgOIINQAUR6gBoDhCDQDFEWoAKI5QA0BxhBoAiiPUAFAcoQaA4gg1ABRHqAGgOEINAMURagAojlADQHGEGgCKI9QAUByhBoDiCDUAFEeoAaA4Qg0AxRFqACiOUANAcYQaAIoj1ABQHKEGgOIINQAUR6gBoDhCDQDFrRhq20+y/Svbv7G90/ZnxzEYAKCxtsM+/5J0VpKHbB8j6Rbb1ye5tefZAADqEOokkfRQ++Ex7Y/0ORQA4KBO56htr7F9h6T9km5Ksm3IPptsL9heWFxcHPGYADC7OoU6yX+TnCZpvaQzbJ8yZJ/NSeaTzA8GgxGPCQCz66iu+khyv6SbJW3sZRoAwGN0uepjYPtp7eMnS3qDpN/3PBcAoNXlqo8TJF1pe42asF+b5Af9jgUAWNLlqo/fSjp9DLMAAIbgzkQAKI5QA0BxhBoAiiPUAFAcoQaA4gg1ABRHqAGgOEINAMURagAojlADQHGEGgCKI9QAUByhBoDiCDUAFEeoAaA4Qg0AxRFqACiOUANAcYQaAIoj1ABQHKEGgOIINQAUR6gBoDhCDQDFEWoAKI5QA0BxhBoAiiPUAFDciqG2fZLtm23vsr3T9oXjGAwA0FjbYZ8Dkj6e5HbbT5W03fZNSXb1PBsAQB2OqJPcneT29vGDknZLOrHvwQAAjaM6R217TtLpkrYN2bbJ9oLthcXFxRGNBwDoHGrbx0n6jqSPJXng0O1JNieZTzI/GAxGOSMAzLROobZ9jJpIX5Xku/2OBABYrstVH5b0NUm7k3yx/5EAAMt1OaJ+laT3STrL9h3tj7f0PBcAoLXi5XlJbpHkMcwCABiCOxMBoDhCDQDFEWoAKI5QA0BxhBoAiiPUAFAcoQaA4gg1ABRHqAGgOEINAMURagAojlADQHGEGgCKI9QAUByhBoDiCDUAFEeoAaA4Qg0AxRFqACiOUANAcYQaAIoj1ABQHKEGgOIINQAUR6gBoDhCDQDFEWoAKI5QA0BxK4ba9hW299veMY6BAACP1uWI+uuSNvY8BwDgMFYMdZKtku4bwywAgCFGdo7a9ibbC7YXFhcXR/WyADDzRhbqJJuTzCeZHwwGo3pZAJh5XPUBAMURagAorsvleVdL+qWkF9jeZ/uC/scCACxZu9IOSd49jkEAAMNx6gMAiiPUAFAcoQaA4gg1ABRHqAGgOEINAMURagAojlADQHGEGgCKI9QAUByhBoDiCDUAFEeoAaA4Qg0AxRFqACiOUANAcYQaAIoj1ABQHKEGgOIINQAUR6gBoDhCDQDFEWoAKI5QA0Bxayc9ANDF3EU/LPX6ey89u6dJgMfiiBoAiuOIGhPR9xFy3zgCxzgRajxuqz264/D/rBFxxxJOfQBAcZ2OqG1vlPRlSWskfTXJpb1OhYniCLkGTq9gyYqhtr1G0mWS3iBpn6TbbG9Jsqvv4TAcIcUwff++4C+CyelyRH2GpD1J7pIk29dIOkfSTISaKAKNin8WZuUvjy6hPlHSX5Z9vE/Syw/dyfYmSZvaDx+y/YfHP97IrJN0z6SHmDDWoME6NKZiHfz5x/XLq63Bcw63YWRXfSTZLGnzqF5vlGwvJJmf9ByTxBo0WIcG67C61qDLVR9/lXTSso/Xt88BAMagS6hvk7TB9nNtP1HSeZK29DsWAGDJiqc+khyw/RFJN6i5PO+KJDt7n2y0Sp6SGTPWoME6NFiHVbQGTjLpGQAAR8CdiQBQHKEGgOKmNtS2v2D797Z/a/s6209btu1i23ts/8H2myY4Zu9sn2t7p+2Hbc8fsm2W1mFj+3nusX3RpOcZJ9tX2N5ve8ey555h+ybbf2x/fvokZ+yb7ZNs32x7V/vn4cL2+VWxDlMbakk3STolyUsk3SnpYkmyfbKaK1deJGmjpMvb2+Sn1Q5J75S0dfmTs7QOy94G4c2STpb07vbznxVfV/P/eLmLJP0kyQZJP2k/nmYHJH08ycmSXiHpw+3vgVWxDlMb6iQ3JjnQfnirmuu/peb292uS/CvJnyTtUXOb/FRKsjvJsLtEZ2kdHnkbhCT/lrT0NggzIclWSfcd8vQ5kq5sH18p6e3jnGncktyd5Pb28YOSdqu563pVrMPUhvoQH5B0fft42C3xJ459osmbpXWYpc+1q+OT3N0+/puk4yc5zDjZnpN0uqRtWiXrsKq/cYDtH0t61pBNlyT5frvPJWq+7LlqnLONU5d1AA4nSWzPxHW6to+T9B1JH0vygO1HtlVeh1Ud6iSvP9J22++X9FZJr8vBC8an7pb4ldbhMKZuHY5glj7Xrv5u+4Qkd9s+QdL+SQ/UN9vHqIn0VUm+2z69KtZhak99tN/s4FOS3pbkn8s2bZF0nu1jbT9X0gZJv5rEjBM2S+vA2yA81hZJ57ePz5c01V95uTl0/pqk3Um+uGzTqliHqb0z0fYeScdKurd96tYkH2q3XaLmvPUBNV8CXT/8VVY/2++Q9BVJA0n3S7ojyZvabbO0Dm+R9CUdfBuEz012ovGxfbWk16h5W8+/S/qMpO9JulbSsyX9WdK7khz6D45Tw/aZkn4u6XeSHm6f/rSa89Tl12FqQw0A02JqT30AwLQg1ABQHKEGgOIINQAUR6gBoDhCDQDFEWoAKO5/D8jiI/s5LfAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(kmf_norm, bins=25)\n",
    "print(len(kmf))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
